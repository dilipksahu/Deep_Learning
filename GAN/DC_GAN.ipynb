{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DC_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5551780a06b4c19a1e193644c629148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b1c62d7ab1b24c0f94f74ae4da817466",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fba8e3cd51584e738048be9174e46c48",
              "IPY_MODEL_289504e6048a4997942266fc396bd7fd"
            ]
          }
        },
        "b1c62d7ab1b24c0f94f74ae4da817466": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fba8e3cd51584e738048be9174e46c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f114e520ef4f4eaf8b419f8976b4da4e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e25edb4cbe04c49ad7ec2ad356423c6"
          }
        },
        "289504e6048a4997942266fc396bd7fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dab797c4535f49caaec42bdb1b5cb205",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 51455305.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f209b174ce54b70980b532021d87264"
          }
        },
        "f114e520ef4f4eaf8b419f8976b4da4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e25edb4cbe04c49ad7ec2ad356423c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dab797c4535f49caaec42bdb1b5cb205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f209b174ce54b70980b532021d87264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf-KFq9_hH5w",
        "colab_type": "code",
        "outputId": "dac3c843-8105-4d5f-f584-9e46847b3693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install pytorch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/67/f403d4ae6e9cd74b546ee88cccdb29b8415a9c1b3d80aebeb20c9ea91d96/pytorch-1.0.2.tar.gz\n",
            "Building wheels for collected packages: pytorch\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "Installing collected packages: pytorch\n",
            "    Running setup.py install for pytorch ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-x0cnx5yj/pytorch/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-x0cnx5yj/pytorch/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-ng8ifb_j/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdsHz8GLhzQM",
        "colab_type": "code",
        "outputId": "9cfb4eb0-23c6-41d3-df5c-4bea9d6968e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5nWfxK7hzgc",
        "colab_type": "code",
        "outputId": "176638ed-a7ac-4a4b-bdcb-8236fae70fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name=\"/content/drive/My Drive/datagan.zip\"\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('done')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EOQjRyBhMv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Setting some hyperparameters\n",
        "batchSize = 64 # We set the size of the batch.\n",
        "imageSize = 64 # We set the size of the generated images (64x64).\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ulukZYphb4r",
        "colab_type": "code",
        "outputId": "a3262238-ff2a-4e5e-db57-7153b1b7efba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142,
          "referenced_widgets": [
            "a5551780a06b4c19a1e193644c629148",
            "b1c62d7ab1b24c0f94f74ae4da817466",
            "fba8e3cd51584e738048be9174e46c48",
            "289504e6048a4997942266fc396bd7fd",
            "f114e520ef4f4eaf8b419f8976b4da4e",
            "6e25edb4cbe04c49ad7ec2ad356423c6",
            "dab797c4535f49caaec42bdb1b5cb205",
            "6f209b174ce54b70980b532021d87264"
          ]
        }
      },
      "source": [
        "# Creating the transformations\n",
        "transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), \n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) \n",
        "# We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n",
        "\n",
        "# Loading the dataset\n",
        "dataset = dset.CIFAR10(root = '/content/datagan', download = True, transform = transform) \n",
        "# We download the training set in the ./data folder and we apply the previous transformations on each image.\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2)\n",
        " # We use dataLoader to get the images of the training set batch by batch.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/datagan/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5551780a06b4c19a1e193644c629148",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/datagan/cifar-10-python.tar.gz to /content/datagan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX-EoDoAoDN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the weights_init function that takes as input a neural network m and that will \n",
        "# initialize all its weights.\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THhGXTGdoKb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the generator\n",
        "\n",
        "class G(nn.Module): # We introduce a class to define the generator.\n",
        "\n",
        "    def __init__(self): # We introduce the __init__() function that will define the architecture of the generator.\n",
        "        super(G, self).__init__() # We inherit from the nn.Module tools.\n",
        "        self.main = nn.Sequential( \n",
        "            # We create a meta module of a neural network that will contain a sequence of modules\n",
        "            # (convolutions, full connections, etc.).\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), # We start with an inversed convolution.\n",
        "            nn.BatchNorm2d(512), # We normalize all the features along the dimension of the batch.\n",
        "            nn.ReLU(True), # We apply a ReLU rectification to break the linearity.\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
        "            nn.BatchNorm2d(256), # We normalize again.\n",
        "            nn.ReLU(True), # We apply another ReLU.\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
        "            nn.BatchNorm2d(128), # We normalize again.\n",
        "            nn.ReLU(True), # We apply another ReLU.\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
        "            nn.BatchNorm2d(64), # We normalize again.\n",
        "            nn.ReLU(True), # We apply another ReLU.\n",
        "            #sound like output of 3 channel.....\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
        "            nn.Tanh() # We apply a Tanh rectification to break the linearity and stay between -1 and +1.\n",
        "        )\n",
        "\n",
        "    def forward(self, input): \n",
        "      # We define the forward function that takes as argument an input that will be fed to the neural network,\n",
        "      # and that will return the output containing the generated images.\n",
        "       \n",
        "        output = self.main(input) \n",
        "        # We forward propagate the signal through the whole neural network of the generator defined by self.main.\n",
        "        return output # We return the output containing the generated images.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uTpuR7SoNdU",
        "colab_type": "code",
        "outputId": "1c9c138c-465b-4782-efe2-2560a8e126f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Creating the generator\n",
        "netG = G() # We create the generator object.\n",
        "netG.apply(weights_init) # We initialize all the weights of its neural network.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "G(\n",
              "  (main): Sequential(\n",
              "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (13): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Jq6Y9ToP_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the discriminator\n",
        "\n",
        "class D(nn.Module): # We introduce a class to define the discriminator.\n",
        "\n",
        "    def __init__(self): # We introduce the __init__() function that will define the architecture of the discriminator.\n",
        "        super(D, self).__init__() # We inherit from the nn.Module tools.\n",
        "        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # We start with a convolution.\n",
        "            nn.LeakyReLU(0.2, inplace = True), # We apply a LeakyReLU.\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False), # We add another convolution.\n",
        "            nn.BatchNorm2d(128), # We normalize all the features along the dimension of the batch.\n",
        "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias = False), # We add another convolution.\n",
        "            nn.BatchNorm2d(256), # We normalize again.\n",
        "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias = False), # We add another convolution.\n",
        "            nn.BatchNorm2d(512), # We normalize again.\n",
        "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias = False), # We add another convolution.\n",
        "            nn.Sigmoid() # We apply a Sigmoid rectification to break the linearity and stay between 0 and 1.\n",
        "        )\n",
        "\n",
        "    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output which will be a value between 0 and 1.\n",
        "        output = self.main(input) # We forward propagate the signal through the whole neural network of the discriminator defined by self.main.\n",
        "        return output.view(-1) # We return the output which will be a value between 0 and 1.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoFrqGytoXL6",
        "colab_type": "code",
        "outputId": "5a185c7d-3750-48e8-b236-86ef3e6d0625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "# Creating the discriminator\n",
        "netD = D() # We create the discriminator object.\n",
        "netD.apply(weights_init) # We initialize all the weights of its neural network.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "D(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (12): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8sdfbSOoauv",
        "colab_type": "code",
        "outputId": "670d5699-8ee4-426d-c9d7-15bcf41d8182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Training the DCGANs\n",
        "\n",
        "criterion = nn.BCELoss() \n",
        "# We create a criterion object that will measure the error between the prediction and the target.\n",
        "\n",
        "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) \n",
        "# We create the optimizer object of the discriminator.\n",
        "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) \n",
        "# We create the optimizer object of the generator.\n",
        "\n",
        "\n",
        "for epoch in range(25):\n",
        "\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
        "\n",
        "        netD.zero_grad()\n",
        "        \n",
        "        # Training the discriminator with a real image of the dataset\n",
        "        real, _ = data\n",
        "        input = Variable(real)\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(input)\n",
        "        errD_real = criterion(output, target)\n",
        "        \n",
        "        # Training the discriminator with a fake image generated by the generator\n",
        "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
        "        fake = netG(noise)\n",
        "        target = Variable(torch.zeros(input.size()[0]))\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, target)\n",
        "        \n",
        "        # Backpropagating the total error\n",
        "        errD = errD_real + errD_fake\n",
        "        errD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # 2nd Step: Updating the weights of the neural network of the generator\n",
        "\n",
        "        netG.zero_grad()\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, target)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "        \n",
        "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data, errG.data))\n",
        "        if i % 100 == 0:\n",
        "            vutils.save_image(real, '%s/real_samples.png' % \"/content/datagan/Result\", normalize = True)\n",
        "            fake = netG(noise)\n",
        "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"/content/datagan/Result\", epoch), normalize = True)  \n",
        "         "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/25][0/782] Loss_D: 1.1351 Loss_G: 5.2437\n",
            "[0/25][1/782] Loss_D: 1.2040 Loss_G: 6.6315\n",
            "[0/25][2/782] Loss_D: 0.7257 Loss_G: 5.7888\n",
            "[0/25][3/782] Loss_D: 1.0678 Loss_G: 6.4337\n",
            "[0/25][4/782] Loss_D: 1.2495 Loss_G: 7.4909\n",
            "[0/25][5/782] Loss_D: 0.8462 Loss_G: 6.8916\n",
            "[0/25][6/782] Loss_D: 1.0439 Loss_G: 7.7404\n",
            "[0/25][7/782] Loss_D: 0.8554 Loss_G: 8.1539\n",
            "[0/25][8/782] Loss_D: 0.5594 Loss_G: 8.0987\n",
            "[0/25][9/782] Loss_D: 0.9320 Loss_G: 9.0477\n",
            "[0/25][10/782] Loss_D: 0.4228 Loss_G: 9.1339\n",
            "[0/25][11/782] Loss_D: 0.4081 Loss_G: 9.2858\n",
            "[0/25][12/782] Loss_D: 0.6733 Loss_G: 10.4738\n",
            "[0/25][13/782] Loss_D: 0.3137 Loss_G: 8.3561\n",
            "[0/25][14/782] Loss_D: 0.6901 Loss_G: 13.4079\n",
            "[0/25][15/782] Loss_D: 0.3799 Loss_G: 10.3876\n",
            "[0/25][16/782] Loss_D: 0.3510 Loss_G: 7.9896\n",
            "[0/25][17/782] Loss_D: 1.4879 Loss_G: 16.8496\n",
            "[0/25][18/782] Loss_D: 0.4224 Loss_G: 16.0810\n",
            "[0/25][19/782] Loss_D: 0.2243 Loss_G: 9.9331\n",
            "[0/25][20/782] Loss_D: 0.6203 Loss_G: 12.2625\n",
            "[0/25][21/782] Loss_D: 0.2485 Loss_G: 9.9064\n",
            "[0/25][22/782] Loss_D: 0.4755 Loss_G: 12.6079\n",
            "[0/25][23/782] Loss_D: 0.1413 Loss_G: 9.8141\n",
            "[0/25][24/782] Loss_D: 0.6991 Loss_G: 17.1320\n",
            "[0/25][25/782] Loss_D: 0.1643 Loss_G: 16.0756\n",
            "[0/25][26/782] Loss_D: 0.1995 Loss_G: 9.3653\n",
            "[0/25][27/782] Loss_D: 1.0724 Loss_G: 19.3177\n",
            "[0/25][28/782] Loss_D: 0.6372 Loss_G: 19.4232\n",
            "[0/25][29/782] Loss_D: 0.0790 Loss_G: 15.3249\n",
            "[0/25][30/782] Loss_D: 0.0561 Loss_G: 7.4274\n",
            "[0/25][31/782] Loss_D: 1.8891 Loss_G: 21.5946\n",
            "[0/25][32/782] Loss_D: 0.1409 Loss_G: 23.6631\n",
            "[0/25][33/782] Loss_D: 0.2882 Loss_G: 19.4240\n",
            "[0/25][34/782] Loss_D: 0.2202 Loss_G: 10.0094\n",
            "[0/25][35/782] Loss_D: 1.3200 Loss_G: 20.2005\n",
            "[0/25][36/782] Loss_D: 0.2418 Loss_G: 21.4572\n",
            "[0/25][37/782] Loss_D: 0.1488 Loss_G: 17.4337\n",
            "[0/25][38/782] Loss_D: 0.1519 Loss_G: 9.1756\n",
            "[0/25][39/782] Loss_D: 0.6151 Loss_G: 16.8958\n",
            "[0/25][40/782] Loss_D: 0.1935 Loss_G: 17.3203\n",
            "[0/25][41/782] Loss_D: 0.1115 Loss_G: 13.4063\n",
            "[0/25][42/782] Loss_D: 0.0617 Loss_G: 7.1493\n",
            "[0/25][43/782] Loss_D: 0.6458 Loss_G: 17.3450\n",
            "[0/25][44/782] Loss_D: 0.2273 Loss_G: 17.9805\n",
            "[0/25][45/782] Loss_D: 0.2790 Loss_G: 14.2522\n",
            "[0/25][46/782] Loss_D: 0.0523 Loss_G: 7.0530\n",
            "[0/25][47/782] Loss_D: 1.0950 Loss_G: 21.0072\n",
            "[0/25][48/782] Loss_D: 0.0679 Loss_G: 23.3328\n",
            "[0/25][49/782] Loss_D: 0.2475 Loss_G: 19.4579\n",
            "[0/25][50/782] Loss_D: 0.0961 Loss_G: 12.4133\n",
            "[0/25][51/782] Loss_D: 0.1069 Loss_G: 5.2007\n",
            "[0/25][52/782] Loss_D: 2.0810 Loss_G: 24.4515\n",
            "[0/25][53/782] Loss_D: 0.4550 Loss_G: 28.7886\n",
            "[0/25][54/782] Loss_D: 0.3484 Loss_G: 28.1645\n",
            "[0/25][55/782] Loss_D: 0.0722 Loss_G: 25.7748\n",
            "[0/25][56/782] Loss_D: 0.0742 Loss_G: 20.7550\n",
            "[0/25][57/782] Loss_D: 0.0493 Loss_G: 14.2683\n",
            "[0/25][58/782] Loss_D: 0.0666 Loss_G: 6.5745\n",
            "[0/25][59/782] Loss_D: 0.2113 Loss_G: 6.9898\n",
            "[0/25][60/782] Loss_D: 0.1558 Loss_G: 8.5564\n",
            "[0/25][61/782] Loss_D: 0.1105 Loss_G: 7.2483\n",
            "[0/25][62/782] Loss_D: 0.2406 Loss_G: 6.0605\n",
            "[0/25][63/782] Loss_D: 0.7802 Loss_G: 15.5742\n",
            "[0/25][64/782] Loss_D: 0.5920 Loss_G: 13.6304\n",
            "[0/25][65/782] Loss_D: 0.1431 Loss_G: 9.8319\n",
            "[0/25][66/782] Loss_D: 0.3344 Loss_G: 7.4674\n",
            "[0/25][67/782] Loss_D: 1.3681 Loss_G: 20.1921\n",
            "[0/25][68/782] Loss_D: 2.4030 Loss_G: 17.1042\n",
            "[0/25][69/782] Loss_D: 0.3906 Loss_G: 9.2202\n",
            "[0/25][70/782] Loss_D: 0.2946 Loss_G: 4.6707\n",
            "[0/25][71/782] Loss_D: 0.8778 Loss_G: 13.2780\n",
            "[0/25][72/782] Loss_D: 0.5735 Loss_G: 12.0537\n",
            "[0/25][73/782] Loss_D: 0.2468 Loss_G: 8.0271\n",
            "[0/25][74/782] Loss_D: 0.1948 Loss_G: 4.3922\n",
            "[0/25][75/782] Loss_D: 0.9862 Loss_G: 10.7617\n",
            "[0/25][76/782] Loss_D: 0.6411 Loss_G: 7.9657\n",
            "[0/25][77/782] Loss_D: 0.1550 Loss_G: 5.6471\n",
            "[0/25][78/782] Loss_D: 0.6376 Loss_G: 8.9177\n",
            "[0/25][79/782] Loss_D: 0.5134 Loss_G: 6.5408\n",
            "[0/25][80/782] Loss_D: 0.2999 Loss_G: 5.8783\n",
            "[0/25][81/782] Loss_D: 0.3970 Loss_G: 8.5290\n",
            "[0/25][82/782] Loss_D: 0.3156 Loss_G: 6.1429\n",
            "[0/25][83/782] Loss_D: 0.4130 Loss_G: 4.4713\n",
            "[0/25][84/782] Loss_D: 0.4612 Loss_G: 9.8268\n",
            "[0/25][85/782] Loss_D: 0.4042 Loss_G: 6.3704\n",
            "[0/25][86/782] Loss_D: 0.3304 Loss_G: 4.0018\n",
            "[0/25][87/782] Loss_D: 1.1585 Loss_G: 11.7357\n",
            "[0/25][88/782] Loss_D: 2.0051 Loss_G: 6.2352\n",
            "[0/25][89/782] Loss_D: 0.5873 Loss_G: 3.5529\n",
            "[0/25][90/782] Loss_D: 0.7797 Loss_G: 7.8413\n",
            "[0/25][91/782] Loss_D: 0.2909 Loss_G: 6.6369\n",
            "[0/25][92/782] Loss_D: 0.5586 Loss_G: 3.5125\n",
            "[0/25][93/782] Loss_D: 0.9939 Loss_G: 9.2968\n",
            "[0/25][94/782] Loss_D: 0.5324 Loss_G: 7.4503\n",
            "[0/25][95/782] Loss_D: 0.3296 Loss_G: 4.6464\n",
            "[0/25][96/782] Loss_D: 0.5205 Loss_G: 5.6519\n",
            "[0/25][97/782] Loss_D: 0.2978 Loss_G: 6.1616\n",
            "[0/25][98/782] Loss_D: 0.6155 Loss_G: 4.8220\n",
            "[0/25][99/782] Loss_D: 0.6430 Loss_G: 9.1183\n",
            "[0/25][100/782] Loss_D: 0.7567 Loss_G: 5.8156\n",
            "[0/25][101/782] Loss_D: 0.2839 Loss_G: 3.9487\n",
            "[0/25][102/782] Loss_D: 0.7056 Loss_G: 9.3660\n",
            "[0/25][103/782] Loss_D: 0.6705 Loss_G: 7.8942\n",
            "[0/25][104/782] Loss_D: 0.1941 Loss_G: 3.7840\n",
            "[0/25][105/782] Loss_D: 1.0250 Loss_G: 10.6894\n",
            "[0/25][106/782] Loss_D: 1.0873 Loss_G: 7.5399\n",
            "[0/25][107/782] Loss_D: 0.2385 Loss_G: 3.8816\n",
            "[0/25][108/782] Loss_D: 0.6036 Loss_G: 8.2636\n",
            "[0/25][109/782] Loss_D: 0.6963 Loss_G: 5.1968\n",
            "[0/25][110/782] Loss_D: 0.2451 Loss_G: 4.8382\n",
            "[0/25][111/782] Loss_D: 0.3560 Loss_G: 6.7202\n",
            "[0/25][112/782] Loss_D: 0.2712 Loss_G: 5.5257\n",
            "[0/25][113/782] Loss_D: 0.2986 Loss_G: 5.4824\n",
            "[0/25][114/782] Loss_D: 0.4757 Loss_G: 6.9715\n",
            "[0/25][115/782] Loss_D: 0.5752 Loss_G: 3.3130\n",
            "[0/25][116/782] Loss_D: 0.9316 Loss_G: 12.5962\n",
            "[0/25][117/782] Loss_D: 2.6198 Loss_G: 7.6631\n",
            "[0/25][118/782] Loss_D: 0.2231 Loss_G: 3.1931\n",
            "[0/25][119/782] Loss_D: 0.5810 Loss_G: 7.0505\n",
            "[0/25][120/782] Loss_D: 0.6292 Loss_G: 4.0755\n",
            "[0/25][121/782] Loss_D: 0.3726 Loss_G: 4.3791\n",
            "[0/25][122/782] Loss_D: 0.3030 Loss_G: 5.7742\n",
            "[0/25][123/782] Loss_D: 0.2377 Loss_G: 4.9915\n",
            "[0/25][124/782] Loss_D: 0.3326 Loss_G: 4.1365\n",
            "[0/25][125/782] Loss_D: 0.4784 Loss_G: 5.4027\n",
            "[0/25][126/782] Loss_D: 0.2248 Loss_G: 5.2251\n",
            "[0/25][127/782] Loss_D: 0.3583 Loss_G: 4.4337\n",
            "[0/25][128/782] Loss_D: 0.3477 Loss_G: 6.5378\n",
            "[0/25][129/782] Loss_D: 0.3476 Loss_G: 4.0633\n",
            "[0/25][130/782] Loss_D: 0.4994 Loss_G: 7.3331\n",
            "[0/25][131/782] Loss_D: 0.3888 Loss_G: 5.1738\n",
            "[0/25][132/782] Loss_D: 0.3773 Loss_G: 7.1998\n",
            "[0/25][133/782] Loss_D: 0.6394 Loss_G: 3.8997\n",
            "[0/25][134/782] Loss_D: 0.8116 Loss_G: 11.6337\n",
            "[0/25][135/782] Loss_D: 1.3162 Loss_G: 6.3759\n",
            "[0/25][136/782] Loss_D: 0.1887 Loss_G: 4.0808\n",
            "[0/25][137/782] Loss_D: 1.2530 Loss_G: 12.1519\n",
            "[0/25][138/782] Loss_D: 2.0260 Loss_G: 7.6391\n",
            "[0/25][139/782] Loss_D: 0.0896 Loss_G: 4.3486\n",
            "[0/25][140/782] Loss_D: 0.3603 Loss_G: 5.4669\n",
            "[0/25][141/782] Loss_D: 0.2758 Loss_G: 6.5343\n",
            "[0/25][142/782] Loss_D: 0.4535 Loss_G: 4.3773\n",
            "[0/25][143/782] Loss_D: 0.6141 Loss_G: 8.0477\n",
            "[0/25][144/782] Loss_D: 0.6875 Loss_G: 5.3549\n",
            "[0/25][145/782] Loss_D: 0.5816 Loss_G: 7.8891\n",
            "[0/25][146/782] Loss_D: 0.2792 Loss_G: 6.7500\n",
            "[0/25][147/782] Loss_D: 0.3329 Loss_G: 7.0792\n",
            "[0/25][148/782] Loss_D: 0.3023 Loss_G: 5.8404\n",
            "[0/25][149/782] Loss_D: 0.3093 Loss_G: 7.2358\n",
            "[0/25][150/782] Loss_D: 0.3624 Loss_G: 5.3705\n",
            "[0/25][151/782] Loss_D: 0.3792 Loss_G: 7.1501\n",
            "[0/25][152/782] Loss_D: 0.2181 Loss_G: 5.9268\n",
            "[0/25][153/782] Loss_D: 0.2721 Loss_G: 5.2143\n",
            "[0/25][154/782] Loss_D: 0.3854 Loss_G: 6.9502\n",
            "[0/25][155/782] Loss_D: 0.1624 Loss_G: 6.2211\n",
            "[0/25][156/782] Loss_D: 0.3207 Loss_G: 6.1828\n",
            "[0/25][157/782] Loss_D: 0.4222 Loss_G: 8.7965\n",
            "[0/25][158/782] Loss_D: 0.1932 Loss_G: 6.9026\n",
            "[0/25][159/782] Loss_D: 0.4615 Loss_G: 11.3230\n",
            "[0/25][160/782] Loss_D: 0.4223 Loss_G: 7.8988\n",
            "[0/25][161/782] Loss_D: 0.2593 Loss_G: 6.2175\n",
            "[0/25][162/782] Loss_D: 0.3247 Loss_G: 11.0244\n",
            "[0/25][163/782] Loss_D: 0.3066 Loss_G: 8.4848\n",
            "[0/25][164/782] Loss_D: 0.1297 Loss_G: 6.6386\n",
            "[0/25][165/782] Loss_D: 0.3742 Loss_G: 8.3978\n",
            "[0/25][166/782] Loss_D: 0.1676 Loss_G: 7.0041\n",
            "[0/25][167/782] Loss_D: 0.2197 Loss_G: 6.8431\n",
            "[0/25][168/782] Loss_D: 0.1921 Loss_G: 5.8605\n",
            "[0/25][169/782] Loss_D: 0.2930 Loss_G: 7.1344\n",
            "[0/25][170/782] Loss_D: 0.1228 Loss_G: 6.3030\n",
            "[0/25][171/782] Loss_D: 0.1431 Loss_G: 5.5650\n",
            "[0/25][172/782] Loss_D: 0.2753 Loss_G: 8.5349\n",
            "[0/25][173/782] Loss_D: 0.2216 Loss_G: 5.9687\n",
            "[0/25][174/782] Loss_D: 0.2556 Loss_G: 5.5151\n",
            "[0/25][175/782] Loss_D: 0.1800 Loss_G: 8.6506\n",
            "[0/25][176/782] Loss_D: 0.0819 Loss_G: 8.0050\n",
            "[0/25][177/782] Loss_D: 0.2155 Loss_G: 4.1528\n",
            "[0/25][178/782] Loss_D: 0.4626 Loss_G: 11.7383\n",
            "[0/25][179/782] Loss_D: 0.1972 Loss_G: 11.7995\n",
            "[0/25][180/782] Loss_D: 0.2222 Loss_G: 8.7943\n",
            "[0/25][181/782] Loss_D: 0.0331 Loss_G: 5.5037\n",
            "[0/25][182/782] Loss_D: 0.3230 Loss_G: 9.0255\n",
            "[0/25][183/782] Loss_D: 0.1295 Loss_G: 6.9795\n",
            "[0/25][184/782] Loss_D: 0.2048 Loss_G: 4.5735\n",
            "[0/25][185/782] Loss_D: 0.9973 Loss_G: 15.7127\n",
            "[0/25][186/782] Loss_D: 5.5658 Loss_G: 12.6563\n",
            "[0/25][187/782] Loss_D: 1.1760 Loss_G: 2.6723\n",
            "[0/25][188/782] Loss_D: 2.3467 Loss_G: 9.5634\n",
            "[0/25][189/782] Loss_D: 0.2302 Loss_G: 10.0209\n",
            "[0/25][190/782] Loss_D: 0.4919 Loss_G: 5.7765\n",
            "[0/25][191/782] Loss_D: 0.3396 Loss_G: 4.7940\n",
            "[0/25][192/782] Loss_D: 0.6568 Loss_G: 7.9113\n",
            "[0/25][193/782] Loss_D: 0.4713 Loss_G: 6.0187\n",
            "[0/25][194/782] Loss_D: 0.2013 Loss_G: 3.6492\n",
            "[0/25][195/782] Loss_D: 0.6442 Loss_G: 6.8078\n",
            "[0/25][196/782] Loss_D: 0.4337 Loss_G: 5.7335\n",
            "[0/25][197/782] Loss_D: 0.1564 Loss_G: 4.5413\n",
            "[0/25][198/782] Loss_D: 0.3166 Loss_G: 5.3555\n",
            "[0/25][199/782] Loss_D: 0.1161 Loss_G: 5.8028\n",
            "[0/25][200/782] Loss_D: 0.1747 Loss_G: 4.6565\n",
            "[0/25][201/782] Loss_D: 0.2137 Loss_G: 4.8429\n",
            "[0/25][202/782] Loss_D: 0.1703 Loss_G: 5.7615\n",
            "[0/25][203/782] Loss_D: 0.4644 Loss_G: 3.6882\n",
            "[0/25][204/782] Loss_D: 0.7085 Loss_G: 9.1218\n",
            "[0/25][205/782] Loss_D: 0.1229 Loss_G: 8.2428\n",
            "[0/25][206/782] Loss_D: 0.2159 Loss_G: 5.2348\n",
            "[0/25][207/782] Loss_D: 0.4997 Loss_G: 8.5642\n",
            "[0/25][208/782] Loss_D: 0.1424 Loss_G: 8.0603\n",
            "[0/25][209/782] Loss_D: 0.1760 Loss_G: 5.5077\n",
            "[0/25][210/782] Loss_D: 0.2185 Loss_G: 5.2095\n",
            "[0/25][211/782] Loss_D: 0.2843 Loss_G: 5.0594\n",
            "[0/25][212/782] Loss_D: 0.1562 Loss_G: 5.0268\n",
            "[0/25][213/782] Loss_D: 0.1994 Loss_G: 5.4513\n",
            "[0/25][214/782] Loss_D: 0.1204 Loss_G: 5.5426\n",
            "[0/25][215/782] Loss_D: 0.1274 Loss_G: 4.8076\n",
            "[0/25][216/782] Loss_D: 0.1786 Loss_G: 5.5212\n",
            "[0/25][217/782] Loss_D: 0.2362 Loss_G: 4.3718\n",
            "[0/25][218/782] Loss_D: 0.1800 Loss_G: 5.7371\n",
            "[0/25][219/782] Loss_D: 0.1413 Loss_G: 5.2515\n",
            "[0/25][220/782] Loss_D: 0.0935 Loss_G: 5.2560\n",
            "[0/25][221/782] Loss_D: 0.1457 Loss_G: 4.9495\n",
            "[0/25][222/782] Loss_D: 0.1834 Loss_G: 6.3977\n",
            "[0/25][223/782] Loss_D: 0.0926 Loss_G: 6.0054\n",
            "[0/25][224/782] Loss_D: 0.1849 Loss_G: 3.8469\n",
            "[0/25][225/782] Loss_D: 0.4325 Loss_G: 10.2293\n",
            "[0/25][226/782] Loss_D: 0.3866 Loss_G: 8.8460\n",
            "[0/25][227/782] Loss_D: 0.0489 Loss_G: 6.1406\n",
            "[0/25][228/782] Loss_D: 0.1249 Loss_G: 5.7607\n",
            "[0/25][229/782] Loss_D: 0.1726 Loss_G: 6.5454\n",
            "[0/25][230/782] Loss_D: 0.0611 Loss_G: 6.3220\n",
            "[0/25][231/782] Loss_D: 0.1393 Loss_G: 4.7761\n",
            "[0/25][232/782] Loss_D: 0.2071 Loss_G: 7.5174\n",
            "[0/25][233/782] Loss_D: 0.4182 Loss_G: 4.1251\n",
            "[0/25][234/782] Loss_D: 0.1940 Loss_G: 6.4249\n",
            "[0/25][235/782] Loss_D: 0.0882 Loss_G: 6.4625\n",
            "[0/25][236/782] Loss_D: 0.0674 Loss_G: 5.8289\n",
            "[0/25][237/782] Loss_D: 0.0644 Loss_G: 5.1114\n",
            "[0/25][238/782] Loss_D: 0.0704 Loss_G: 5.7583\n",
            "[0/25][239/782] Loss_D: 0.0478 Loss_G: 5.8861\n",
            "[0/25][240/782] Loss_D: 0.0535 Loss_G: 5.4770\n",
            "[0/25][241/782] Loss_D: 0.1172 Loss_G: 4.5588\n",
            "[0/25][242/782] Loss_D: 0.3171 Loss_G: 7.3605\n",
            "[0/25][243/782] Loss_D: 0.3159 Loss_G: 3.9707\n",
            "[0/25][244/782] Loss_D: 0.1622 Loss_G: 7.0049\n",
            "[0/25][245/782] Loss_D: 0.0888 Loss_G: 6.7105\n",
            "[0/25][246/782] Loss_D: 0.0621 Loss_G: 5.4949\n",
            "[0/25][247/782] Loss_D: 0.1316 Loss_G: 5.0953\n",
            "[0/25][248/782] Loss_D: 0.1188 Loss_G: 6.9110\n",
            "[0/25][249/782] Loss_D: 0.2316 Loss_G: 3.3142\n",
            "[0/25][250/782] Loss_D: 0.1928 Loss_G: 8.4642\n",
            "[0/25][251/782] Loss_D: 0.1090 Loss_G: 8.2661\n",
            "[0/25][252/782] Loss_D: 0.0625 Loss_G: 6.6084\n",
            "[0/25][253/782] Loss_D: 0.1129 Loss_G: 6.2460\n",
            "[0/25][254/782] Loss_D: 0.1784 Loss_G: 8.9197\n",
            "[0/25][255/782] Loss_D: 0.1098 Loss_G: 7.6226\n",
            "[0/25][256/782] Loss_D: 0.4547 Loss_G: 13.5259\n",
            "[0/25][257/782] Loss_D: 1.0588 Loss_G: 5.2957\n",
            "[0/25][258/782] Loss_D: 0.8052 Loss_G: 20.6507\n",
            "[0/25][259/782] Loss_D: 1.4291 Loss_G: 17.4955\n",
            "[0/25][260/782] Loss_D: 0.0527 Loss_G: 13.1931\n",
            "[0/25][261/782] Loss_D: 0.0442 Loss_G: 7.3657\n",
            "[0/25][262/782] Loss_D: 1.2331 Loss_G: 16.1200\n",
            "[0/25][263/782] Loss_D: 2.9809 Loss_G: 8.2286\n",
            "[0/25][264/782] Loss_D: 0.3759 Loss_G: 6.0046\n",
            "[0/25][265/782] Loss_D: 1.2927 Loss_G: 14.0639\n",
            "[0/25][266/782] Loss_D: 1.0194 Loss_G: 10.9392\n",
            "[0/25][267/782] Loss_D: 0.3873 Loss_G: 6.3609\n",
            "[0/25][268/782] Loss_D: 0.6817 Loss_G: 6.3270\n",
            "[0/25][269/782] Loss_D: 0.3910 Loss_G: 8.4302\n",
            "[0/25][270/782] Loss_D: 0.2626 Loss_G: 7.5763\n",
            "[0/25][271/782] Loss_D: 0.2631 Loss_G: 4.7882\n",
            "[0/25][272/782] Loss_D: 0.2577 Loss_G: 6.1938\n",
            "[0/25][273/782] Loss_D: 0.2049 Loss_G: 5.6007\n",
            "[0/25][274/782] Loss_D: 0.1744 Loss_G: 5.4357\n",
            "[0/25][275/782] Loss_D: 0.1163 Loss_G: 5.5730\n",
            "[0/25][276/782] Loss_D: 0.3842 Loss_G: 3.5248\n",
            "[0/25][277/782] Loss_D: 0.4219 Loss_G: 7.1680\n",
            "[0/25][278/782] Loss_D: 0.1892 Loss_G: 6.4714\n",
            "[0/25][279/782] Loss_D: 0.1042 Loss_G: 5.3091\n",
            "[0/25][280/782] Loss_D: 0.2015 Loss_G: 4.2067\n",
            "[0/25][281/782] Loss_D: 0.3160 Loss_G: 6.1192\n",
            "[0/25][282/782] Loss_D: 0.2203 Loss_G: 5.3418\n",
            "[0/25][283/782] Loss_D: 0.1923 Loss_G: 5.2755\n",
            "[0/25][284/782] Loss_D: 0.1740 Loss_G: 5.5260\n",
            "[0/25][285/782] Loss_D: 0.2634 Loss_G: 4.7412\n",
            "[0/25][286/782] Loss_D: 0.5860 Loss_G: 7.8344\n",
            "[0/25][287/782] Loss_D: 0.1804 Loss_G: 7.2525\n",
            "[0/25][288/782] Loss_D: 0.5209 Loss_G: 2.9010\n",
            "[0/25][289/782] Loss_D: 0.9033 Loss_G: 10.9424\n",
            "[0/25][290/782] Loss_D: 0.4647 Loss_G: 11.8966\n",
            "[0/25][291/782] Loss_D: 0.8237 Loss_G: 8.1096\n",
            "[0/25][292/782] Loss_D: 0.0569 Loss_G: 5.0590\n",
            "[0/25][293/782] Loss_D: 0.1565 Loss_G: 4.9970\n",
            "[0/25][294/782] Loss_D: 0.2031 Loss_G: 7.0109\n",
            "[0/25][295/782] Loss_D: 0.1590 Loss_G: 6.2144\n",
            "[0/25][296/782] Loss_D: 0.1897 Loss_G: 5.0098\n",
            "[0/25][297/782] Loss_D: 0.2825 Loss_G: 7.1805\n",
            "[0/25][298/782] Loss_D: 0.2187 Loss_G: 6.2330\n",
            "[0/25][299/782] Loss_D: 0.1919 Loss_G: 5.4742\n",
            "[0/25][300/782] Loss_D: 0.1919 Loss_G: 5.8563\n",
            "[0/25][301/782] Loss_D: 0.1669 Loss_G: 5.6283\n",
            "[0/25][302/782] Loss_D: 0.1541 Loss_G: 5.3684\n",
            "[0/25][303/782] Loss_D: 0.1900 Loss_G: 5.5804\n",
            "[0/25][304/782] Loss_D: 0.0537 Loss_G: 5.9106\n",
            "[0/25][305/782] Loss_D: 0.0962 Loss_G: 5.6324\n",
            "[0/25][306/782] Loss_D: 0.0784 Loss_G: 5.5140\n",
            "[0/25][307/782] Loss_D: 0.1179 Loss_G: 6.5475\n",
            "[0/25][308/782] Loss_D: 0.2379 Loss_G: 5.3600\n",
            "[0/25][309/782] Loss_D: 0.1200 Loss_G: 5.9614\n",
            "[0/25][310/782] Loss_D: 0.2027 Loss_G: 6.7170\n",
            "[0/25][311/782] Loss_D: 0.2405 Loss_G: 5.7104\n",
            "[0/25][312/782] Loss_D: 0.1032 Loss_G: 6.0299\n",
            "[0/25][313/782] Loss_D: 0.1340 Loss_G: 6.6142\n",
            "[0/25][314/782] Loss_D: 0.1164 Loss_G: 6.0778\n",
            "[0/25][315/782] Loss_D: 0.1672 Loss_G: 5.2278\n",
            "[0/25][316/782] Loss_D: 0.2072 Loss_G: 8.4539\n",
            "[0/25][317/782] Loss_D: 0.0950 Loss_G: 9.1049\n",
            "[0/25][318/782] Loss_D: 0.0266 Loss_G: 7.4334\n",
            "[0/25][319/782] Loss_D: 0.0947 Loss_G: 4.7717\n",
            "[0/25][320/782] Loss_D: 0.1483 Loss_G: 7.2489\n",
            "[0/25][321/782] Loss_D: 0.1921 Loss_G: 6.2618\n",
            "[0/25][322/782] Loss_D: 0.0978 Loss_G: 5.0129\n",
            "[0/25][323/782] Loss_D: 0.1812 Loss_G: 6.5709\n",
            "[0/25][324/782] Loss_D: 0.0672 Loss_G: 6.5767\n",
            "[0/25][325/782] Loss_D: 0.0556 Loss_G: 5.8851\n",
            "[0/25][326/782] Loss_D: 0.0702 Loss_G: 6.1531\n",
            "[0/25][327/782] Loss_D: 0.0850 Loss_G: 7.2021\n",
            "[0/25][328/782] Loss_D: 0.0393 Loss_G: 7.4047\n",
            "[0/25][329/782] Loss_D: 0.1340 Loss_G: 8.1197\n",
            "[0/25][330/782] Loss_D: 0.0680 Loss_G: 7.8768\n",
            "[0/25][331/782] Loss_D: 0.1594 Loss_G: 8.9236\n",
            "[0/25][332/782] Loss_D: 0.0839 Loss_G: 7.6583\n",
            "[0/25][333/782] Loss_D: 0.1142 Loss_G: 7.7888\n",
            "[0/25][334/782] Loss_D: 0.0992 Loss_G: 7.7160\n",
            "[0/25][335/782] Loss_D: 0.1135 Loss_G: 8.0017\n",
            "[0/25][336/782] Loss_D: 0.0465 Loss_G: 7.5623\n",
            "[0/25][337/782] Loss_D: 0.0533 Loss_G: 7.9224\n",
            "[0/25][338/782] Loss_D: 0.0564 Loss_G: 7.1405\n",
            "[0/25][339/782] Loss_D: 0.1070 Loss_G: 7.7493\n",
            "[0/25][340/782] Loss_D: 0.1842 Loss_G: 5.5721\n",
            "[0/25][341/782] Loss_D: 0.3718 Loss_G: 17.5271\n",
            "[0/25][342/782] Loss_D: 0.7644 Loss_G: 13.9145\n",
            "[0/25][343/782] Loss_D: 0.0059 Loss_G: 7.5117\n",
            "[0/25][344/782] Loss_D: 0.3873 Loss_G: 10.9616\n",
            "[0/25][345/782] Loss_D: 0.0528 Loss_G: 9.7344\n",
            "[0/25][346/782] Loss_D: 0.2476 Loss_G: 7.6595\n",
            "[0/25][347/782] Loss_D: 0.1653 Loss_G: 9.7035\n",
            "[0/25][348/782] Loss_D: 0.1042 Loss_G: 7.5785\n",
            "[0/25][349/782] Loss_D: 0.0957 Loss_G: 7.8060\n",
            "[0/25][350/782] Loss_D: 0.1709 Loss_G: 6.4250\n",
            "[0/25][351/782] Loss_D: 0.2885 Loss_G: 10.2868\n",
            "[0/25][352/782] Loss_D: 0.1336 Loss_G: 8.3795\n",
            "[0/25][353/782] Loss_D: 0.1021 Loss_G: 5.9233\n",
            "[0/25][354/782] Loss_D: 0.1743 Loss_G: 10.4282\n",
            "[0/25][355/782] Loss_D: 0.1105 Loss_G: 9.5907\n",
            "[0/25][356/782] Loss_D: 0.0882 Loss_G: 6.0630\n",
            "[0/25][357/782] Loss_D: 0.1359 Loss_G: 6.7397\n",
            "[0/25][358/782] Loss_D: 0.1272 Loss_G: 7.1248\n",
            "[0/25][359/782] Loss_D: 0.0592 Loss_G: 7.2704\n",
            "[0/25][360/782] Loss_D: 0.1275 Loss_G: 7.0122\n",
            "[0/25][361/782] Loss_D: 0.1422 Loss_G: 9.4334\n",
            "[0/25][362/782] Loss_D: 0.1518 Loss_G: 6.6381\n",
            "[0/25][363/782] Loss_D: 0.7748 Loss_G: 24.4832\n",
            "[0/25][364/782] Loss_D: 6.9704 Loss_G: 17.9245\n",
            "[0/25][365/782] Loss_D: 0.5228 Loss_G: 10.5331\n",
            "[0/25][366/782] Loss_D: 0.1031 Loss_G: 4.6004\n",
            "[0/25][367/782] Loss_D: 0.9151 Loss_G: 15.3315\n",
            "[0/25][368/782] Loss_D: 0.7360 Loss_G: 13.8245\n",
            "[0/25][369/782] Loss_D: 0.2558 Loss_G: 10.4738\n",
            "[0/25][370/782] Loss_D: 0.1082 Loss_G: 7.7828\n",
            "[0/25][371/782] Loss_D: 0.0887 Loss_G: 6.7171\n",
            "[0/25][372/782] Loss_D: 0.1669 Loss_G: 7.4514\n",
            "[0/25][373/782] Loss_D: 0.1661 Loss_G: 7.2306\n",
            "[0/25][374/782] Loss_D: 0.1553 Loss_G: 5.9800\n",
            "[0/25][375/782] Loss_D: 0.3226 Loss_G: 8.6873\n",
            "[0/25][376/782] Loss_D: 0.1963 Loss_G: 6.7515\n",
            "[0/25][377/782] Loss_D: 0.2705 Loss_G: 4.6871\n",
            "[0/25][378/782] Loss_D: 0.5879 Loss_G: 11.1611\n",
            "[0/25][379/782] Loss_D: 0.3678 Loss_G: 10.1633\n",
            "[0/25][380/782] Loss_D: 0.3385 Loss_G: 6.4892\n",
            "[0/25][381/782] Loss_D: 0.2149 Loss_G: 7.8966\n",
            "[0/25][382/782] Loss_D: 0.1637 Loss_G: 7.8847\n",
            "[0/25][383/782] Loss_D: 0.0666 Loss_G: 7.5558\n",
            "[0/25][384/782] Loss_D: 0.0718 Loss_G: 6.1951\n",
            "[0/25][385/782] Loss_D: 0.0876 Loss_G: 6.7170\n",
            "[0/25][386/782] Loss_D: 0.1879 Loss_G: 5.9400\n",
            "[0/25][387/782] Loss_D: 0.3599 Loss_G: 6.7734\n",
            "[0/25][388/782] Loss_D: 0.0815 Loss_G: 7.4603\n",
            "[0/25][389/782] Loss_D: 0.1475 Loss_G: 9.4586\n",
            "[0/25][390/782] Loss_D: 0.2747 Loss_G: 5.3027\n",
            "[0/25][391/782] Loss_D: 0.4853 Loss_G: 15.6491\n",
            "[0/25][392/782] Loss_D: 0.8248 Loss_G: 11.6236\n",
            "[0/25][393/782] Loss_D: 0.1470 Loss_G: 5.7309\n",
            "[0/25][394/782] Loss_D: 0.4115 Loss_G: 10.8697\n",
            "[0/25][395/782] Loss_D: 0.0459 Loss_G: 10.7251\n",
            "[0/25][396/782] Loss_D: 0.4275 Loss_G: 7.5597\n",
            "[0/25][397/782] Loss_D: 0.1332 Loss_G: 4.8295\n",
            "[0/25][398/782] Loss_D: 0.3649 Loss_G: 9.3852\n",
            "[0/25][399/782] Loss_D: 0.1187 Loss_G: 9.1407\n",
            "[0/25][400/782] Loss_D: 0.0753 Loss_G: 6.8648\n",
            "[0/25][401/782] Loss_D: 0.0947 Loss_G: 5.1909\n",
            "[0/25][402/782] Loss_D: 0.1819 Loss_G: 7.0229\n",
            "[0/25][403/782] Loss_D: 0.1715 Loss_G: 6.6374\n",
            "[0/25][404/782] Loss_D: 0.0585 Loss_G: 6.1827\n",
            "[0/25][405/782] Loss_D: 0.2202 Loss_G: 5.5857\n",
            "[0/25][406/782] Loss_D: 0.2803 Loss_G: 11.2118\n",
            "[0/25][407/782] Loss_D: 0.1074 Loss_G: 11.3732\n",
            "[0/25][408/782] Loss_D: 0.2686 Loss_G: 7.8950\n",
            "[0/25][409/782] Loss_D: 0.1048 Loss_G: 6.8033\n",
            "[0/25][410/782] Loss_D: 0.1731 Loss_G: 10.9824\n",
            "[0/25][411/782] Loss_D: 0.1402 Loss_G: 10.4239\n",
            "[0/25][412/782] Loss_D: 0.1507 Loss_G: 7.2989\n",
            "[0/25][413/782] Loss_D: 0.1458 Loss_G: 5.6476\n",
            "[0/25][414/782] Loss_D: 0.2690 Loss_G: 10.7114\n",
            "[0/25][415/782] Loss_D: 0.1296 Loss_G: 10.0411\n",
            "[0/25][416/782] Loss_D: 0.1817 Loss_G: 6.1214\n",
            "[0/25][417/782] Loss_D: 0.1057 Loss_G: 5.1453\n",
            "[0/25][418/782] Loss_D: 0.2427 Loss_G: 9.3309\n",
            "[0/25][419/782] Loss_D: 0.1923 Loss_G: 8.2414\n",
            "[0/25][420/782] Loss_D: 0.0922 Loss_G: 5.9390\n",
            "[0/25][421/782] Loss_D: 0.3265 Loss_G: 9.4486\n",
            "[0/25][422/782] Loss_D: 0.3498 Loss_G: 7.0335\n",
            "[0/25][423/782] Loss_D: 0.5463 Loss_G: 12.2726\n",
            "[0/25][424/782] Loss_D: 0.9926 Loss_G: 8.6914\n",
            "[0/25][425/782] Loss_D: 0.1478 Loss_G: 7.6841\n",
            "[0/25][426/782] Loss_D: 0.0950 Loss_G: 7.9002\n",
            "[0/25][427/782] Loss_D: 0.0780 Loss_G: 6.8451\n",
            "[0/25][428/782] Loss_D: 0.1670 Loss_G: 7.9070\n",
            "[0/25][429/782] Loss_D: 0.1023 Loss_G: 6.3215\n",
            "[0/25][430/782] Loss_D: 0.0912 Loss_G: 7.1339\n",
            "[0/25][431/782] Loss_D: 0.1427 Loss_G: 5.9525\n",
            "[0/25][432/782] Loss_D: 0.0675 Loss_G: 6.2765\n",
            "[0/25][433/782] Loss_D: 0.0874 Loss_G: 5.9067\n",
            "[0/25][434/782] Loss_D: 0.0831 Loss_G: 5.7724\n",
            "[0/25][435/782] Loss_D: 0.0689 Loss_G: 6.2976\n",
            "[0/25][436/782] Loss_D: 0.1015 Loss_G: 5.2758\n",
            "[0/25][437/782] Loss_D: 0.1514 Loss_G: 8.3476\n",
            "[0/25][438/782] Loss_D: 0.0202 Loss_G: 8.6159\n",
            "[0/25][439/782] Loss_D: 0.0657 Loss_G: 6.4287\n",
            "[0/25][440/782] Loss_D: 0.1290 Loss_G: 6.3729\n",
            "[0/25][441/782] Loss_D: 0.1359 Loss_G: 6.3188\n",
            "[0/25][442/782] Loss_D: 0.1351 Loss_G: 6.3343\n",
            "[0/25][443/782] Loss_D: 0.0803 Loss_G: 7.1507\n",
            "[0/25][444/782] Loss_D: 0.0462 Loss_G: 6.6972\n",
            "[0/25][445/782] Loss_D: 0.0694 Loss_G: 7.7771\n",
            "[0/25][446/782] Loss_D: 0.0614 Loss_G: 6.8101\n",
            "[0/25][447/782] Loss_D: 0.1319 Loss_G: 5.7081\n",
            "[0/25][448/782] Loss_D: 0.1213 Loss_G: 9.2067\n",
            "[0/25][449/782] Loss_D: 0.0257 Loss_G: 8.4256\n",
            "[0/25][450/782] Loss_D: 0.0108 Loss_G: 7.2617\n",
            "[0/25][451/782] Loss_D: 0.0332 Loss_G: 5.8555\n",
            "[0/25][452/782] Loss_D: 0.0704 Loss_G: 6.0788\n",
            "[0/25][453/782] Loss_D: 0.0662 Loss_G: 6.7391\n",
            "[0/25][454/782] Loss_D: 0.0731 Loss_G: 5.8066\n",
            "[0/25][455/782] Loss_D: 0.0538 Loss_G: 5.5858\n",
            "[0/25][456/782] Loss_D: 0.1561 Loss_G: 5.5088\n",
            "[0/25][457/782] Loss_D: 0.1012 Loss_G: 8.2621\n",
            "[0/25][458/782] Loss_D: 0.0134 Loss_G: 8.3476\n",
            "[0/25][459/782] Loss_D: 0.1438 Loss_G: 5.8979\n",
            "[0/25][460/782] Loss_D: 0.5479 Loss_G: 22.2124\n",
            "[0/25][461/782] Loss_D: 1.7249 Loss_G: 20.9224\n",
            "[0/25][462/782] Loss_D: 0.2264 Loss_G: 15.6643\n",
            "[0/25][463/782] Loss_D: 0.0136 Loss_G: 5.2993\n",
            "[0/25][464/782] Loss_D: 1.6922 Loss_G: 19.8336\n",
            "[0/25][465/782] Loss_D: 2.9127 Loss_G: 15.8716\n",
            "[0/25][466/782] Loss_D: 0.2195 Loss_G: 10.9623\n",
            "[0/25][467/782] Loss_D: 0.0802 Loss_G: 7.7627\n",
            "[0/25][468/782] Loss_D: 0.1393 Loss_G: 5.5677\n",
            "[0/25][469/782] Loss_D: 0.5168 Loss_G: 8.7103\n",
            "[0/25][470/782] Loss_D: 0.3550 Loss_G: 6.5689\n",
            "[0/25][471/782] Loss_D: 0.1588 Loss_G: 5.3840\n",
            "[0/25][472/782] Loss_D: 0.3461 Loss_G: 7.5932\n",
            "[0/25][473/782] Loss_D: 0.2122 Loss_G: 6.9834\n",
            "[0/25][474/782] Loss_D: 0.0955 Loss_G: 5.1650\n",
            "[0/25][475/782] Loss_D: 0.3706 Loss_G: 5.3797\n",
            "[0/25][476/782] Loss_D: 0.2559 Loss_G: 6.9187\n",
            "[0/25][477/782] Loss_D: 0.2384 Loss_G: 5.3302\n",
            "[0/25][478/782] Loss_D: 1.2338 Loss_G: 15.9454\n",
            "[0/25][479/782] Loss_D: 5.7841 Loss_G: 7.3711\n",
            "[0/25][480/782] Loss_D: 1.5033 Loss_G: 4.1854\n",
            "[0/25][481/782] Loss_D: 1.0443 Loss_G: 8.6094\n",
            "[0/25][482/782] Loss_D: 0.1121 Loss_G: 8.1810\n",
            "[0/25][483/782] Loss_D: 0.2602 Loss_G: 5.6746\n",
            "[0/25][484/782] Loss_D: 0.4261 Loss_G: 5.8320\n",
            "[0/25][485/782] Loss_D: 0.3367 Loss_G: 5.7638\n",
            "[0/25][486/782] Loss_D: 0.3819 Loss_G: 4.9326\n",
            "[0/25][487/782] Loss_D: 0.2566 Loss_G: 5.2682\n",
            "[0/25][488/782] Loss_D: 0.2645 Loss_G: 5.5845\n",
            "[0/25][489/782] Loss_D: 0.2702 Loss_G: 4.9619\n",
            "[0/25][490/782] Loss_D: 0.2238 Loss_G: 5.0699\n",
            "[0/25][491/782] Loss_D: 0.3661 Loss_G: 6.2825\n",
            "[0/25][492/782] Loss_D: 0.3598 Loss_G: 4.3605\n",
            "[0/25][493/782] Loss_D: 0.4963 Loss_G: 7.2815\n",
            "[0/25][494/782] Loss_D: 0.4372 Loss_G: 4.4317\n",
            "[0/25][495/782] Loss_D: 0.4991 Loss_G: 5.3144\n",
            "[0/25][496/782] Loss_D: 0.4213 Loss_G: 7.1947\n",
            "[0/25][497/782] Loss_D: 1.0162 Loss_G: 1.9698\n",
            "[0/25][498/782] Loss_D: 1.3209 Loss_G: 10.3620\n",
            "[0/25][499/782] Loss_D: 0.4834 Loss_G: 10.1496\n",
            "[0/25][500/782] Loss_D: 0.2954 Loss_G: 5.9780\n",
            "[0/25][501/782] Loss_D: 0.1115 Loss_G: 3.5702\n",
            "[0/25][502/782] Loss_D: 0.5832 Loss_G: 7.3798\n",
            "[0/25][503/782] Loss_D: 0.1973 Loss_G: 6.9053\n",
            "[0/25][504/782] Loss_D: 0.4452 Loss_G: 4.6309\n",
            "[0/25][505/782] Loss_D: 0.1801 Loss_G: 4.3061\n",
            "[0/25][506/782] Loss_D: 0.3837 Loss_G: 5.3968\n",
            "[0/25][507/782] Loss_D: 0.2613 Loss_G: 4.3706\n",
            "[0/25][508/782] Loss_D: 0.3285 Loss_G: 5.4457\n",
            "[0/25][509/782] Loss_D: 0.1908 Loss_G: 4.7971\n",
            "[0/25][510/782] Loss_D: 0.4248 Loss_G: 3.8866\n",
            "[0/25][511/782] Loss_D: 0.4328 Loss_G: 6.6796\n",
            "[0/25][512/782] Loss_D: 0.3055 Loss_G: 4.5260\n",
            "[0/25][513/782] Loss_D: 0.1793 Loss_G: 3.9934\n",
            "[0/25][514/782] Loss_D: 0.3616 Loss_G: 9.5136\n",
            "[0/25][515/782] Loss_D: 0.2516 Loss_G: 9.0996\n",
            "[0/25][516/782] Loss_D: 0.2458 Loss_G: 6.2704\n",
            "[0/25][517/782] Loss_D: 0.1331 Loss_G: 4.5560\n",
            "[0/25][518/782] Loss_D: 0.1873 Loss_G: 7.0636\n",
            "[0/25][519/782] Loss_D: 0.0479 Loss_G: 6.9421\n",
            "[0/25][520/782] Loss_D: 0.1816 Loss_G: 5.1979\n",
            "[0/25][521/782] Loss_D: 0.2535 Loss_G: 5.9294\n",
            "[0/25][522/782] Loss_D: 0.2289 Loss_G: 5.5829\n",
            "[0/25][523/782] Loss_D: 0.1550 Loss_G: 5.7710\n",
            "[0/25][524/782] Loss_D: 0.2917 Loss_G: 4.5692\n",
            "[0/25][525/782] Loss_D: 0.1695 Loss_G: 5.0695\n",
            "[0/25][526/782] Loss_D: 0.1864 Loss_G: 5.9550\n",
            "[0/25][527/782] Loss_D: 0.1241 Loss_G: 5.6735\n",
            "[0/25][528/782] Loss_D: 0.1300 Loss_G: 5.0316\n",
            "[0/25][529/782] Loss_D: 0.1570 Loss_G: 6.8935\n",
            "[0/25][530/782] Loss_D: 0.1599 Loss_G: 6.2009\n",
            "[0/25][531/782] Loss_D: 0.1134 Loss_G: 5.5153\n",
            "[0/25][532/782] Loss_D: 0.2514 Loss_G: 7.3823\n",
            "[0/25][533/782] Loss_D: 0.1193 Loss_G: 6.3185\n",
            "[0/25][534/782] Loss_D: 0.2477 Loss_G: 3.5187\n",
            "[0/25][535/782] Loss_D: 0.6158 Loss_G: 12.7798\n",
            "[0/25][536/782] Loss_D: 1.1849 Loss_G: 9.4801\n",
            "[0/25][537/782] Loss_D: 0.0781 Loss_G: 5.7782\n",
            "[0/25][538/782] Loss_D: 0.1993 Loss_G: 6.0421\n",
            "[0/25][539/782] Loss_D: 0.0983 Loss_G: 6.2609\n",
            "[0/25][540/782] Loss_D: 0.2310 Loss_G: 6.1119\n",
            "[0/25][541/782] Loss_D: 0.3386 Loss_G: 4.8321\n",
            "[0/25][542/782] Loss_D: 0.4007 Loss_G: 6.7357\n",
            "[0/25][543/782] Loss_D: 0.3311 Loss_G: 4.4397\n",
            "[0/25][544/782] Loss_D: 0.1364 Loss_G: 5.3873\n",
            "[0/25][545/782] Loss_D: 0.0620 Loss_G: 6.0970\n",
            "[0/25][546/782] Loss_D: 0.0977 Loss_G: 5.3867\n",
            "[0/25][547/782] Loss_D: 0.0895 Loss_G: 5.1839\n",
            "[0/25][548/782] Loss_D: 0.1144 Loss_G: 4.3362\n",
            "[0/25][549/782] Loss_D: 0.1020 Loss_G: 4.7557\n",
            "[0/25][550/782] Loss_D: 0.0737 Loss_G: 5.4665\n",
            "[0/25][551/782] Loss_D: 0.1509 Loss_G: 4.4024\n",
            "[0/25][552/782] Loss_D: 0.1298 Loss_G: 4.8559\n",
            "[0/25][553/782] Loss_D: 0.1125 Loss_G: 5.1193\n",
            "[0/25][554/782] Loss_D: 0.1062 Loss_G: 5.1469\n",
            "[0/25][555/782] Loss_D: 0.2298 Loss_G: 3.7025\n",
            "[0/25][556/782] Loss_D: 0.3754 Loss_G: 7.3293\n",
            "[0/25][557/782] Loss_D: 0.3863 Loss_G: 4.8771\n",
            "[0/25][558/782] Loss_D: 0.0674 Loss_G: 4.4532\n",
            "[0/25][559/782] Loss_D: 0.1699 Loss_G: 5.3395\n",
            "[0/25][560/782] Loss_D: 0.1223 Loss_G: 5.0782\n",
            "[0/25][561/782] Loss_D: 0.2319 Loss_G: 3.9855\n",
            "[0/25][562/782] Loss_D: 0.2588 Loss_G: 5.4912\n",
            "[0/25][563/782] Loss_D: 0.3024 Loss_G: 3.9808\n",
            "[0/25][564/782] Loss_D: 0.1680 Loss_G: 4.4198\n",
            "[0/25][565/782] Loss_D: 0.1639 Loss_G: 4.8882\n",
            "[0/25][566/782] Loss_D: 0.3599 Loss_G: 4.8359\n",
            "[0/25][567/782] Loss_D: 0.4317 Loss_G: 4.1856\n",
            "[0/25][568/782] Loss_D: 0.5688 Loss_G: 11.5371\n",
            "[0/25][569/782] Loss_D: 3.8481 Loss_G: 1.3556\n",
            "[0/25][570/782] Loss_D: 2.2634 Loss_G: 10.6872\n",
            "[0/25][571/782] Loss_D: 1.3794 Loss_G: 5.5382\n",
            "[0/25][572/782] Loss_D: 0.5186 Loss_G: 2.1324\n",
            "[0/25][573/782] Loss_D: 1.0714 Loss_G: 5.3460\n",
            "[0/25][574/782] Loss_D: 0.3556 Loss_G: 5.4666\n",
            "[0/25][575/782] Loss_D: 0.3435 Loss_G: 3.9762\n",
            "[0/25][576/782] Loss_D: 0.2280 Loss_G: 3.9749\n",
            "[0/25][577/782] Loss_D: 0.2742 Loss_G: 3.6271\n",
            "[0/25][578/782] Loss_D: 0.2349 Loss_G: 3.7830\n",
            "[0/25][579/782] Loss_D: 0.2079 Loss_G: 4.4913\n",
            "[0/25][580/782] Loss_D: 0.3352 Loss_G: 3.8849\n",
            "[0/25][581/782] Loss_D: 0.2885 Loss_G: 3.3737\n",
            "[0/25][582/782] Loss_D: 0.3155 Loss_G: 4.3183\n",
            "[0/25][583/782] Loss_D: 0.2593 Loss_G: 4.0748\n",
            "[0/25][584/782] Loss_D: 0.2837 Loss_G: 4.0771\n",
            "[0/25][585/782] Loss_D: 0.1884 Loss_G: 4.6124\n",
            "[0/25][586/782] Loss_D: 0.0930 Loss_G: 4.9552\n",
            "[0/25][587/782] Loss_D: 0.2025 Loss_G: 4.6867\n",
            "[0/25][588/782] Loss_D: 0.4610 Loss_G: 2.9299\n",
            "[0/25][589/782] Loss_D: 0.7073 Loss_G: 10.3407\n",
            "[0/25][590/782] Loss_D: 0.7424 Loss_G: 8.4686\n",
            "[0/25][591/782] Loss_D: 0.1278 Loss_G: 4.8589\n",
            "[0/25][592/782] Loss_D: 0.1612 Loss_G: 4.2617\n",
            "[0/25][593/782] Loss_D: 0.5112 Loss_G: 7.9444\n",
            "[0/25][594/782] Loss_D: 0.5562 Loss_G: 5.0134\n",
            "[0/25][595/782] Loss_D: 0.1366 Loss_G: 3.2159\n",
            "[0/25][596/782] Loss_D: 0.7915 Loss_G: 8.1232\n",
            "[0/25][597/782] Loss_D: 0.7538 Loss_G: 3.6415\n",
            "[0/25][598/782] Loss_D: 0.5672 Loss_G: 6.2464\n",
            "[0/25][599/782] Loss_D: 0.2154 Loss_G: 6.2573\n",
            "[0/25][600/782] Loss_D: 0.3953 Loss_G: 4.5123\n",
            "[0/25][601/782] Loss_D: 0.3584 Loss_G: 5.9772\n",
            "[0/25][602/782] Loss_D: 0.1118 Loss_G: 5.5676\n",
            "[0/25][603/782] Loss_D: 0.2016 Loss_G: 3.9094\n",
            "[0/25][604/782] Loss_D: 0.4214 Loss_G: 8.6784\n",
            "[0/25][605/782] Loss_D: 0.5963 Loss_G: 4.7936\n",
            "[0/25][606/782] Loss_D: 0.2138 Loss_G: 4.2601\n",
            "[0/25][607/782] Loss_D: 0.7555 Loss_G: 11.5549\n",
            "[0/25][608/782] Loss_D: 0.6154 Loss_G: 6.9919\n",
            "[0/25][609/782] Loss_D: 0.1604 Loss_G: 3.0239\n",
            "[0/25][610/782] Loss_D: 0.6428 Loss_G: 10.1130\n",
            "[0/25][611/782] Loss_D: 1.5009 Loss_G: 3.3381\n",
            "[0/25][612/782] Loss_D: 0.4902 Loss_G: 5.7406\n",
            "[0/25][613/782] Loss_D: 0.1131 Loss_G: 6.1264\n",
            "[0/25][614/782] Loss_D: 0.1141 Loss_G: 5.0860\n",
            "[0/25][615/782] Loss_D: 0.1357 Loss_G: 4.4189\n",
            "[0/25][616/782] Loss_D: 0.3076 Loss_G: 3.5526\n",
            "[0/25][617/782] Loss_D: 0.2508 Loss_G: 4.7429\n",
            "[0/25][618/782] Loss_D: 0.1230 Loss_G: 5.2979\n",
            "[0/25][619/782] Loss_D: 0.1580 Loss_G: 4.6169\n",
            "[0/25][620/782] Loss_D: 0.0936 Loss_G: 4.5763\n",
            "[0/25][621/782] Loss_D: 0.1240 Loss_G: 5.2790\n",
            "[0/25][622/782] Loss_D: 0.0830 Loss_G: 5.4312\n",
            "[0/25][623/782] Loss_D: 0.1054 Loss_G: 4.8720\n",
            "[0/25][624/782] Loss_D: 0.1323 Loss_G: 5.1681\n",
            "[0/25][625/782] Loss_D: 0.1386 Loss_G: 4.8247\n",
            "[0/25][626/782] Loss_D: 0.1453 Loss_G: 4.2844\n",
            "[0/25][627/782] Loss_D: 0.1536 Loss_G: 5.0851\n",
            "[0/25][628/782] Loss_D: 0.0778 Loss_G: 5.3801\n",
            "[0/25][629/782] Loss_D: 0.1371 Loss_G: 4.4734\n",
            "[0/25][630/782] Loss_D: 0.1926 Loss_G: 5.4927\n",
            "[0/25][631/782] Loss_D: 0.2183 Loss_G: 5.1136\n",
            "[0/25][632/782] Loss_D: 0.1534 Loss_G: 4.7328\n",
            "[0/25][633/782] Loss_D: 0.3435 Loss_G: 6.6039\n",
            "[0/25][634/782] Loss_D: 0.3138 Loss_G: 4.8568\n",
            "[0/25][635/782] Loss_D: 0.4462 Loss_G: 3.0514\n",
            "[0/25][636/782] Loss_D: 1.0183 Loss_G: 13.4309\n",
            "[0/25][637/782] Loss_D: 0.6380 Loss_G: 12.6019\n",
            "[0/25][638/782] Loss_D: 0.9493 Loss_G: 4.8117\n",
            "[0/25][639/782] Loss_D: 3.3593 Loss_G: 10.8902\n",
            "[0/25][640/782] Loss_D: 0.9662 Loss_G: 9.1226\n",
            "[0/25][641/782] Loss_D: 0.8592 Loss_G: 4.9391\n",
            "[0/25][642/782] Loss_D: 0.2029 Loss_G: 3.5186\n",
            "[0/25][643/782] Loss_D: 0.5180 Loss_G: 5.3033\n",
            "[0/25][644/782] Loss_D: 0.3911 Loss_G: 4.5701\n",
            "[0/25][645/782] Loss_D: 0.3768 Loss_G: 4.1674\n",
            "[0/25][646/782] Loss_D: 0.2677 Loss_G: 4.2682\n",
            "[0/25][647/782] Loss_D: 0.5745 Loss_G: 4.4002\n",
            "[0/25][648/782] Loss_D: 0.7030 Loss_G: 2.7783\n",
            "[0/25][649/782] Loss_D: 0.7157 Loss_G: 5.5911\n",
            "[0/25][650/782] Loss_D: 0.6198 Loss_G: 3.7317\n",
            "[0/25][651/782] Loss_D: 0.4080 Loss_G: 3.6797\n",
            "[0/25][652/782] Loss_D: 0.4046 Loss_G: 6.1201\n",
            "[0/25][653/782] Loss_D: 0.5454 Loss_G: 2.3664\n",
            "[0/25][654/782] Loss_D: 0.6296 Loss_G: 7.5913\n",
            "[0/25][655/782] Loss_D: 0.7111 Loss_G: 2.9409\n",
            "[0/25][656/782] Loss_D: 0.8089 Loss_G: 5.5870\n",
            "[0/25][657/782] Loss_D: 0.7253 Loss_G: 2.4599\n",
            "[0/25][658/782] Loss_D: 0.7277 Loss_G: 6.1027\n",
            "[0/25][659/782] Loss_D: 0.6476 Loss_G: 2.8274\n",
            "[0/25][660/782] Loss_D: 0.4545 Loss_G: 4.3140\n",
            "[0/25][661/782] Loss_D: 0.3106 Loss_G: 3.6812\n",
            "[0/25][662/782] Loss_D: 0.3487 Loss_G: 5.7632\n",
            "[0/25][663/782] Loss_D: 0.2695 Loss_G: 4.3904\n",
            "[0/25][664/782] Loss_D: 0.3882 Loss_G: 1.8928\n",
            "[0/25][665/782] Loss_D: 1.1300 Loss_G: 9.0474\n",
            "[0/25][666/782] Loss_D: 2.3845 Loss_G: 4.4839\n",
            "[0/25][667/782] Loss_D: 0.3444 Loss_G: 1.8721\n",
            "[0/25][668/782] Loss_D: 0.8273 Loss_G: 5.6142\n",
            "[0/25][669/782] Loss_D: 0.5985 Loss_G: 2.7102\n",
            "[0/25][670/782] Loss_D: 0.7820 Loss_G: 5.4883\n",
            "[0/25][671/782] Loss_D: 0.4450 Loss_G: 4.4018\n",
            "[0/25][672/782] Loss_D: 0.6180 Loss_G: 2.0497\n",
            "[0/25][673/782] Loss_D: 0.7819 Loss_G: 7.2278\n",
            "[0/25][674/782] Loss_D: 0.8951 Loss_G: 1.0992\n",
            "[0/25][675/782] Loss_D: 1.3645 Loss_G: 10.2471\n",
            "[0/25][676/782] Loss_D: 2.5237 Loss_G: 5.1880\n",
            "[0/25][677/782] Loss_D: 0.1612 Loss_G: 2.1983\n",
            "[0/25][678/782] Loss_D: 0.7866 Loss_G: 5.4751\n",
            "[0/25][679/782] Loss_D: 0.2752 Loss_G: 4.4884\n",
            "[0/25][680/782] Loss_D: 0.4537 Loss_G: 2.9086\n",
            "[0/25][681/782] Loss_D: 0.5912 Loss_G: 3.8168\n",
            "[0/25][682/782] Loss_D: 0.2985 Loss_G: 4.3221\n",
            "[0/25][683/782] Loss_D: 0.3835 Loss_G: 3.3328\n",
            "[0/25][684/782] Loss_D: 0.7478 Loss_G: 5.2672\n",
            "[0/25][685/782] Loss_D: 0.7649 Loss_G: 3.1565\n",
            "[0/25][686/782] Loss_D: 0.4646 Loss_G: 4.5965\n",
            "[0/25][687/782] Loss_D: 0.1352 Loss_G: 5.2234\n",
            "[0/25][688/782] Loss_D: 0.4630 Loss_G: 2.9996\n",
            "[0/25][689/782] Loss_D: 0.2945 Loss_G: 4.4273\n",
            "[0/25][690/782] Loss_D: 0.2449 Loss_G: 4.5841\n",
            "[0/25][691/782] Loss_D: 0.2786 Loss_G: 3.8924\n",
            "[0/25][692/782] Loss_D: 0.1510 Loss_G: 4.1104\n",
            "[0/25][693/782] Loss_D: 0.1387 Loss_G: 4.6985\n",
            "[0/25][694/782] Loss_D: 0.1731 Loss_G: 4.7522\n",
            "[0/25][695/782] Loss_D: 0.2502 Loss_G: 4.1197\n",
            "[0/25][696/782] Loss_D: 0.1936 Loss_G: 4.8418\n",
            "[0/25][697/782] Loss_D: 0.2604 Loss_G: 4.5669\n",
            "[0/25][698/782] Loss_D: 0.2002 Loss_G: 4.1748\n",
            "[0/25][699/782] Loss_D: 0.1486 Loss_G: 5.0611\n",
            "[0/25][700/782] Loss_D: 0.1320 Loss_G: 6.0810\n",
            "[0/25][701/782] Loss_D: 0.2630 Loss_G: 4.6609\n",
            "[0/25][702/782] Loss_D: 0.5384 Loss_G: 7.1843\n",
            "[0/25][703/782] Loss_D: 0.6666 Loss_G: 2.9533\n",
            "[0/25][704/782] Loss_D: 1.2895 Loss_G: 10.5929\n",
            "[0/25][705/782] Loss_D: 1.1039 Loss_G: 7.5267\n",
            "[0/25][706/782] Loss_D: 0.2133 Loss_G: 3.5372\n",
            "[0/25][707/782] Loss_D: 0.5484 Loss_G: 6.7437\n",
            "[0/25][708/782] Loss_D: 0.1945 Loss_G: 6.1307\n",
            "[0/25][709/782] Loss_D: 0.1790 Loss_G: 4.1467\n",
            "[0/25][710/782] Loss_D: 0.2319 Loss_G: 4.0069\n",
            "[0/25][711/782] Loss_D: 0.3687 Loss_G: 5.8407\n",
            "[0/25][712/782] Loss_D: 0.1595 Loss_G: 5.2957\n",
            "[0/25][713/782] Loss_D: 0.2062 Loss_G: 4.6352\n",
            "[0/25][714/782] Loss_D: 0.5323 Loss_G: 7.5483\n",
            "[0/25][715/782] Loss_D: 0.7171 Loss_G: 2.5574\n",
            "[0/25][716/782] Loss_D: 1.4645 Loss_G: 14.9093\n",
            "[0/25][717/782] Loss_D: 2.1923 Loss_G: 11.9894\n",
            "[0/25][718/782] Loss_D: 0.4597 Loss_G: 8.6166\n",
            "[0/25][719/782] Loss_D: 0.1095 Loss_G: 4.3778\n",
            "[0/25][720/782] Loss_D: 0.7079 Loss_G: 5.3293\n",
            "[0/25][721/782] Loss_D: 0.2557 Loss_G: 6.1468\n",
            "[0/25][722/782] Loss_D: 0.2024 Loss_G: 5.6687\n",
            "[0/25][723/782] Loss_D: 0.3362 Loss_G: 4.0848\n",
            "[0/25][724/782] Loss_D: 0.3703 Loss_G: 4.6069\n",
            "[0/25][725/782] Loss_D: 0.3033 Loss_G: 5.2196\n",
            "[0/25][726/782] Loss_D: 0.3365 Loss_G: 4.4252\n",
            "[0/25][727/782] Loss_D: 0.5348 Loss_G: 2.9298\n",
            "[0/25][728/782] Loss_D: 0.8372 Loss_G: 6.3635\n",
            "[0/25][729/782] Loss_D: 0.4843 Loss_G: 4.2130\n",
            "[0/25][730/782] Loss_D: 0.3155 Loss_G: 3.0854\n",
            "[0/25][731/782] Loss_D: 0.3110 Loss_G: 3.9698\n",
            "[0/25][732/782] Loss_D: 0.2438 Loss_G: 5.2400\n",
            "[0/25][733/782] Loss_D: 0.6831 Loss_G: 1.2274\n",
            "[0/25][734/782] Loss_D: 1.5141 Loss_G: 9.7170\n",
            "[0/25][735/782] Loss_D: 3.0109 Loss_G: 3.7987\n",
            "[0/25][736/782] Loss_D: 0.5490 Loss_G: 2.2624\n",
            "[0/25][737/782] Loss_D: 1.1816 Loss_G: 8.4872\n",
            "[0/25][738/782] Loss_D: 0.9193 Loss_G: 5.0783\n",
            "[0/25][739/782] Loss_D: 0.3854 Loss_G: 1.6267\n",
            "[0/25][740/782] Loss_D: 1.2438 Loss_G: 7.2593\n",
            "[0/25][741/782] Loss_D: 0.7235 Loss_G: 6.6433\n",
            "[0/25][742/782] Loss_D: 0.2356 Loss_G: 4.5966\n",
            "[0/25][743/782] Loss_D: 0.1144 Loss_G: 3.6839\n",
            "[0/25][744/782] Loss_D: 0.2038 Loss_G: 4.3351\n",
            "[0/25][745/782] Loss_D: 0.1859 Loss_G: 4.6539\n",
            "[0/25][746/782] Loss_D: 0.1379 Loss_G: 4.4422\n",
            "[0/25][747/782] Loss_D: 0.1746 Loss_G: 4.1197\n",
            "[0/25][748/782] Loss_D: 0.2097 Loss_G: 4.7538\n",
            "[0/25][749/782] Loss_D: 0.1168 Loss_G: 4.8578\n",
            "[0/25][750/782] Loss_D: 0.1487 Loss_G: 4.0929\n",
            "[0/25][751/782] Loss_D: 0.1548 Loss_G: 3.7585\n",
            "[0/25][752/782] Loss_D: 0.1689 Loss_G: 4.3067\n",
            "[0/25][753/782] Loss_D: 0.1114 Loss_G: 4.5995\n",
            "[0/25][754/782] Loss_D: 0.3084 Loss_G: 4.2992\n",
            "[0/25][755/782] Loss_D: 0.4103 Loss_G: 3.5846\n",
            "[0/25][756/782] Loss_D: 0.4417 Loss_G: 4.5750\n",
            "[0/25][757/782] Loss_D: 0.2020 Loss_G: 5.1420\n",
            "[0/25][758/782] Loss_D: 0.3642 Loss_G: 3.9349\n",
            "[0/25][759/782] Loss_D: 0.4258 Loss_G: 4.1593\n",
            "[0/25][760/782] Loss_D: 0.3522 Loss_G: 4.1402\n",
            "[0/25][761/782] Loss_D: 0.1645 Loss_G: 4.7888\n",
            "[0/25][762/782] Loss_D: 0.3333 Loss_G: 5.4459\n",
            "[0/25][763/782] Loss_D: 0.2904 Loss_G: 4.2650\n",
            "[0/25][764/782] Loss_D: 0.4563 Loss_G: 4.4696\n",
            "[0/25][765/782] Loss_D: 0.2093 Loss_G: 4.8963\n",
            "[0/25][766/782] Loss_D: 0.1474 Loss_G: 5.1623\n",
            "[0/25][767/782] Loss_D: 0.1703 Loss_G: 4.6622\n",
            "[0/25][768/782] Loss_D: 0.1964 Loss_G: 5.5665\n",
            "[0/25][769/782] Loss_D: 0.1134 Loss_G: 5.5213\n",
            "[0/25][770/782] Loss_D: 0.1335 Loss_G: 5.7179\n",
            "[0/25][771/782] Loss_D: 0.1087 Loss_G: 5.1582\n",
            "[0/25][772/782] Loss_D: 0.1734 Loss_G: 5.4811\n",
            "[0/25][773/782] Loss_D: 0.2601 Loss_G: 4.5364\n",
            "[0/25][774/782] Loss_D: 0.2131 Loss_G: 6.4267\n",
            "[0/25][775/782] Loss_D: 0.5234 Loss_G: 3.6203\n",
            "[0/25][776/782] Loss_D: 0.5946 Loss_G: 6.3986\n",
            "[0/25][777/782] Loss_D: 0.4425 Loss_G: 4.0387\n",
            "[0/25][778/782] Loss_D: 0.5991 Loss_G: 4.9845\n",
            "[0/25][779/782] Loss_D: 0.3426 Loss_G: 4.7645\n",
            "[0/25][780/782] Loss_D: 0.5263 Loss_G: 5.1905\n",
            "[0/25][781/782] Loss_D: 0.4367 Loss_G: 2.9614\n",
            "[1/25][0/782] Loss_D: 1.2484 Loss_G: 8.8635\n",
            "[1/25][1/782] Loss_D: 1.4415 Loss_G: 4.2458\n",
            "[1/25][2/782] Loss_D: 0.7483 Loss_G: 4.2039\n",
            "[1/25][3/782] Loss_D: 1.3218 Loss_G: 11.9685\n",
            "[1/25][4/782] Loss_D: 2.7683 Loss_G: 5.6844\n",
            "[1/25][5/782] Loss_D: 0.3198 Loss_G: 3.1550\n",
            "[1/25][6/782] Loss_D: 0.7679 Loss_G: 7.0952\n",
            "[1/25][7/782] Loss_D: 0.5865 Loss_G: 4.1798\n",
            "[1/25][8/782] Loss_D: 0.4589 Loss_G: 5.3546\n",
            "[1/25][9/782] Loss_D: 0.2174 Loss_G: 4.7759\n",
            "[1/25][10/782] Loss_D: 0.4284 Loss_G: 2.5048\n",
            "[1/25][11/782] Loss_D: 1.0741 Loss_G: 7.2561\n",
            "[1/25][12/782] Loss_D: 1.3963 Loss_G: 3.6078\n",
            "[1/25][13/782] Loss_D: 0.2911 Loss_G: 3.0923\n",
            "[1/25][14/782] Loss_D: 0.7953 Loss_G: 7.6777\n",
            "[1/25][15/782] Loss_D: 1.3991 Loss_G: 2.6576\n",
            "[1/25][16/782] Loss_D: 0.9364 Loss_G: 6.5075\n",
            "[1/25][17/782] Loss_D: 0.3534 Loss_G: 4.9733\n",
            "[1/25][18/782] Loss_D: 0.2810 Loss_G: 4.5236\n",
            "[1/25][19/782] Loss_D: 0.4314 Loss_G: 5.1232\n",
            "[1/25][20/782] Loss_D: 0.5620 Loss_G: 3.0726\n",
            "[1/25][21/782] Loss_D: 0.5766 Loss_G: 6.1264\n",
            "[1/25][22/782] Loss_D: 0.2467 Loss_G: 5.5020\n",
            "[1/25][23/782] Loss_D: 0.2728 Loss_G: 5.5437\n",
            "[1/25][24/782] Loss_D: 0.1217 Loss_G: 5.3436\n",
            "[1/25][25/782] Loss_D: 0.3068 Loss_G: 3.4234\n",
            "[1/25][26/782] Loss_D: 0.7838 Loss_G: 5.1040\n",
            "[1/25][27/782] Loss_D: 0.6636 Loss_G: 4.6644\n",
            "[1/25][28/782] Loss_D: 0.2821 Loss_G: 4.8583\n",
            "[1/25][29/782] Loss_D: 0.3737 Loss_G: 9.3852\n",
            "[1/25][30/782] Loss_D: 0.4842 Loss_G: 5.2211\n",
            "[1/25][31/782] Loss_D: 0.3386 Loss_G: 6.4322\n",
            "[1/25][32/782] Loss_D: 0.1358 Loss_G: 5.7805\n",
            "[1/25][33/782] Loss_D: 0.2173 Loss_G: 6.4418\n",
            "[1/25][34/782] Loss_D: 0.2388 Loss_G: 4.8846\n",
            "[1/25][35/782] Loss_D: 0.1489 Loss_G: 4.7989\n",
            "[1/25][36/782] Loss_D: 0.0479 Loss_G: 5.3853\n",
            "[1/25][37/782] Loss_D: 0.1667 Loss_G: 5.3529\n",
            "[1/25][38/782] Loss_D: 0.1640 Loss_G: 5.0607\n",
            "[1/25][39/782] Loss_D: 0.5017 Loss_G: 2.3140\n",
            "[1/25][40/782] Loss_D: 1.2367 Loss_G: 12.9611\n",
            "[1/25][41/782] Loss_D: 1.7098 Loss_G: 5.8986\n",
            "[1/25][42/782] Loss_D: 0.5189 Loss_G: 5.8803\n",
            "[1/25][43/782] Loss_D: 0.7371 Loss_G: 10.5690\n",
            "[1/25][44/782] Loss_D: 2.1429 Loss_G: 1.9752\n",
            "[1/25][45/782] Loss_D: 2.0308 Loss_G: 7.8318\n",
            "[1/25][46/782] Loss_D: 2.2076 Loss_G: 2.4509\n",
            "[1/25][47/782] Loss_D: 1.3371 Loss_G: 4.4654\n",
            "[1/25][48/782] Loss_D: 1.3640 Loss_G: 2.7350\n",
            "[1/25][49/782] Loss_D: 0.9794 Loss_G: 3.4727\n",
            "[1/25][50/782] Loss_D: 0.5512 Loss_G: 4.0612\n",
            "[1/25][51/782] Loss_D: 0.3960 Loss_G: 3.2876\n",
            "[1/25][52/782] Loss_D: 0.5064 Loss_G: 3.1742\n",
            "[1/25][53/782] Loss_D: 0.6135 Loss_G: 4.1499\n",
            "[1/25][54/782] Loss_D: 0.6275 Loss_G: 3.6122\n",
            "[1/25][55/782] Loss_D: 0.5349 Loss_G: 3.5433\n",
            "[1/25][56/782] Loss_D: 0.3721 Loss_G: 3.9001\n",
            "[1/25][57/782] Loss_D: 0.2882 Loss_G: 3.5403\n",
            "[1/25][58/782] Loss_D: 0.4149 Loss_G: 5.1094\n",
            "[1/25][59/782] Loss_D: 0.1674 Loss_G: 5.1385\n",
            "[1/25][60/782] Loss_D: 0.6847 Loss_G: 0.8566\n",
            "[1/25][61/782] Loss_D: 1.4218 Loss_G: 8.8910\n",
            "[1/25][62/782] Loss_D: 0.7746 Loss_G: 6.6076\n",
            "[1/25][63/782] Loss_D: 0.3404 Loss_G: 2.9355\n",
            "[1/25][64/782] Loss_D: 0.7366 Loss_G: 6.1040\n",
            "[1/25][65/782] Loss_D: 0.3010 Loss_G: 4.6824\n",
            "[1/25][66/782] Loss_D: 0.4943 Loss_G: 4.1392\n",
            "[1/25][67/782] Loss_D: 0.9357 Loss_G: 2.2824\n",
            "[1/25][68/782] Loss_D: 0.8469 Loss_G: 7.0533\n",
            "[1/25][69/782] Loss_D: 0.6509 Loss_G: 4.8824\n",
            "[1/25][70/782] Loss_D: 0.2852 Loss_G: 2.3233\n",
            "[1/25][71/782] Loss_D: 0.8442 Loss_G: 7.3707\n",
            "[1/25][72/782] Loss_D: 1.2875 Loss_G: 1.6335\n",
            "[1/25][73/782] Loss_D: 1.2176 Loss_G: 6.5200\n",
            "[1/25][74/782] Loss_D: 0.4343 Loss_G: 3.9397\n",
            "[1/25][75/782] Loss_D: 0.4041 Loss_G: 2.5935\n",
            "[1/25][76/782] Loss_D: 0.5380 Loss_G: 4.8156\n",
            "[1/25][77/782] Loss_D: 0.4730 Loss_G: 4.1485\n",
            "[1/25][78/782] Loss_D: 0.4276 Loss_G: 2.6858\n",
            "[1/25][79/782] Loss_D: 0.7793 Loss_G: 5.1309\n",
            "[1/25][80/782] Loss_D: 0.9204 Loss_G: 1.9737\n",
            "[1/25][81/782] Loss_D: 0.5300 Loss_G: 3.7950\n",
            "[1/25][82/782] Loss_D: 0.3888 Loss_G: 5.2082\n",
            "[1/25][83/782] Loss_D: 0.3825 Loss_G: 3.7381\n",
            "[1/25][84/782] Loss_D: 0.1622 Loss_G: 3.7619\n",
            "[1/25][85/782] Loss_D: 0.3453 Loss_G: 3.6587\n",
            "[1/25][86/782] Loss_D: 0.2321 Loss_G: 4.4209\n",
            "[1/25][87/782] Loss_D: 0.1954 Loss_G: 4.4602\n",
            "[1/25][88/782] Loss_D: 0.3178 Loss_G: 3.0404\n",
            "[1/25][89/782] Loss_D: 0.5717 Loss_G: 5.8262\n",
            "[1/25][90/782] Loss_D: 0.6112 Loss_G: 3.8394\n",
            "[1/25][91/782] Loss_D: 0.0551 Loss_G: 4.2568\n",
            "[1/25][92/782] Loss_D: 0.2620 Loss_G: 4.6083\n",
            "[1/25][93/782] Loss_D: 0.0701 Loss_G: 5.5253\n",
            "[1/25][94/782] Loss_D: 0.0725 Loss_G: 4.9340\n",
            "[1/25][95/782] Loss_D: 0.0797 Loss_G: 4.2926\n",
            "[1/25][96/782] Loss_D: 0.3625 Loss_G: 5.6149\n",
            "[1/25][97/782] Loss_D: 0.5439 Loss_G: 2.9149\n",
            "[1/25][98/782] Loss_D: 0.9278 Loss_G: 8.9642\n",
            "[1/25][99/782] Loss_D: 0.4540 Loss_G: 6.2644\n",
            "[1/25][100/782] Loss_D: 0.2304 Loss_G: 3.4954\n",
            "[1/25][101/782] Loss_D: 0.7805 Loss_G: 9.1478\n",
            "[1/25][102/782] Loss_D: 0.6779 Loss_G: 5.3650\n",
            "[1/25][103/782] Loss_D: 0.3681 Loss_G: 3.0979\n",
            "[1/25][104/782] Loss_D: 0.9445 Loss_G: 10.3542\n",
            "[1/25][105/782] Loss_D: 1.3145 Loss_G: 4.5307\n",
            "[1/25][106/782] Loss_D: 0.4141 Loss_G: 4.3789\n",
            "[1/25][107/782] Loss_D: 0.9669 Loss_G: 7.3818\n",
            "[1/25][108/782] Loss_D: 1.2223 Loss_G: 3.0267\n",
            "[1/25][109/782] Loss_D: 1.3037 Loss_G: 7.1599\n",
            "[1/25][110/782] Loss_D: 0.6334 Loss_G: 4.0336\n",
            "[1/25][111/782] Loss_D: 1.0119 Loss_G: 6.1107\n",
            "[1/25][112/782] Loss_D: 1.8298 Loss_G: 1.5991\n",
            "[1/25][113/782] Loss_D: 1.0588 Loss_G: 5.9806\n",
            "[1/25][114/782] Loss_D: 0.4120 Loss_G: 4.6429\n",
            "[1/25][115/782] Loss_D: 0.6090 Loss_G: 4.0073\n",
            "[1/25][116/782] Loss_D: 0.6620 Loss_G: 4.3816\n",
            "[1/25][117/782] Loss_D: 0.8351 Loss_G: 7.3389\n",
            "[1/25][118/782] Loss_D: 1.7965 Loss_G: 1.5302\n",
            "[1/25][119/782] Loss_D: 2.1280 Loss_G: 7.6720\n",
            "[1/25][120/782] Loss_D: 0.6843 Loss_G: 2.6498\n",
            "[1/25][121/782] Loss_D: 1.0253 Loss_G: 9.9574\n",
            "[1/25][122/782] Loss_D: 2.7252 Loss_G: 2.1795\n",
            "[1/25][123/782] Loss_D: 1.3127 Loss_G: 4.5695\n",
            "[1/25][124/782] Loss_D: 0.7235 Loss_G: 2.9789\n",
            "[1/25][125/782] Loss_D: 1.1643 Loss_G: 2.3272\n",
            "[1/25][126/782] Loss_D: 1.2621 Loss_G: 4.5821\n",
            "[1/25][127/782] Loss_D: 1.0622 Loss_G: 2.5153\n",
            "[1/25][128/782] Loss_D: 0.7608 Loss_G: 3.5282\n",
            "[1/25][129/782] Loss_D: 0.6038 Loss_G: 4.7694\n",
            "[1/25][130/782] Loss_D: 0.4507 Loss_G: 3.5499\n",
            "[1/25][131/782] Loss_D: 0.6083 Loss_G: 4.1280\n",
            "[1/25][132/782] Loss_D: 0.4442 Loss_G: 4.8296\n",
            "[1/25][133/782] Loss_D: 0.5187 Loss_G: 3.8497\n",
            "[1/25][134/782] Loss_D: 0.3698 Loss_G: 3.9114\n",
            "[1/25][135/782] Loss_D: 0.5547 Loss_G: 6.4901\n",
            "[1/25][136/782] Loss_D: 0.7560 Loss_G: 3.4221\n",
            "[1/25][137/782] Loss_D: 0.5848 Loss_G: 6.6359\n",
            "[1/25][138/782] Loss_D: 0.2747 Loss_G: 3.4919\n",
            "[1/25][139/782] Loss_D: 0.6066 Loss_G: 7.3468\n",
            "[1/25][140/782] Loss_D: 0.8620 Loss_G: 2.3932\n",
            "[1/25][141/782] Loss_D: 0.9406 Loss_G: 8.4488\n",
            "[1/25][142/782] Loss_D: 0.5988 Loss_G: 3.7715\n",
            "[1/25][143/782] Loss_D: 0.5124 Loss_G: 3.1151\n",
            "[1/25][144/782] Loss_D: 1.3340 Loss_G: 6.5062\n",
            "[1/25][145/782] Loss_D: 0.6551 Loss_G: 3.6070\n",
            "[1/25][146/782] Loss_D: 0.3984 Loss_G: 1.8149\n",
            "[1/25][147/782] Loss_D: 0.5760 Loss_G: 5.2908\n",
            "[1/25][148/782] Loss_D: 0.3742 Loss_G: 4.2057\n",
            "[1/25][149/782] Loss_D: 0.5779 Loss_G: 1.6105\n",
            "[1/25][150/782] Loss_D: 0.6466 Loss_G: 4.2944\n",
            "[1/25][151/782] Loss_D: 0.2721 Loss_G: 4.2163\n",
            "[1/25][152/782] Loss_D: 0.6474 Loss_G: 1.8107\n",
            "[1/25][153/782] Loss_D: 0.5847 Loss_G: 4.0458\n",
            "[1/25][154/782] Loss_D: 0.4508 Loss_G: 4.2905\n",
            "[1/25][155/782] Loss_D: 0.4963 Loss_G: 2.6188\n",
            "[1/25][156/782] Loss_D: 0.7936 Loss_G: 4.7599\n",
            "[1/25][157/782] Loss_D: 0.4835 Loss_G: 3.8537\n",
            "[1/25][158/782] Loss_D: 0.3826 Loss_G: 2.6204\n",
            "[1/25][159/782] Loss_D: 0.4580 Loss_G: 4.5715\n",
            "[1/25][160/782] Loss_D: 0.3358 Loss_G: 3.6649\n",
            "[1/25][161/782] Loss_D: 0.4541 Loss_G: 3.2655\n",
            "[1/25][162/782] Loss_D: 0.2185 Loss_G: 4.2494\n",
            "[1/25][163/782] Loss_D: 0.2812 Loss_G: 3.5203\n",
            "[1/25][164/782] Loss_D: 0.3059 Loss_G: 3.1566\n",
            "[1/25][165/782] Loss_D: 0.5401 Loss_G: 5.1747\n",
            "[1/25][166/782] Loss_D: 0.5007 Loss_G: 2.6615\n",
            "[1/25][167/782] Loss_D: 0.5608 Loss_G: 4.3692\n",
            "[1/25][168/782] Loss_D: 0.3029 Loss_G: 2.3531\n",
            "[1/25][169/782] Loss_D: 0.6740 Loss_G: 5.7584\n",
            "[1/25][170/782] Loss_D: 0.8627 Loss_G: 0.1931\n",
            "[1/25][171/782] Loss_D: 3.0907 Loss_G: 9.1374\n",
            "[1/25][172/782] Loss_D: 1.9439 Loss_G: 6.6832\n",
            "[1/25][173/782] Loss_D: 0.5322 Loss_G: 2.3582\n",
            "[1/25][174/782] Loss_D: 0.7405 Loss_G: 3.5179\n",
            "[1/25][175/782] Loss_D: 0.4454 Loss_G: 5.0828\n",
            "[1/25][176/782] Loss_D: 0.6549 Loss_G: 3.1383\n",
            "[1/25][177/782] Loss_D: 0.5244 Loss_G: 3.2765\n",
            "[1/25][178/782] Loss_D: 0.6306 Loss_G: 4.1194\n",
            "[1/25][179/782] Loss_D: 0.8499 Loss_G: 2.4012\n",
            "[1/25][180/782] Loss_D: 1.0066 Loss_G: 5.1578\n",
            "[1/25][181/782] Loss_D: 0.6582 Loss_G: 3.4831\n",
            "[1/25][182/782] Loss_D: 0.4766 Loss_G: 3.3723\n",
            "[1/25][183/782] Loss_D: 0.8679 Loss_G: 5.5171\n",
            "[1/25][184/782] Loss_D: 0.5303 Loss_G: 3.4958\n",
            "[1/25][185/782] Loss_D: 0.4701 Loss_G: 5.1927\n",
            "[1/25][186/782] Loss_D: 0.5119 Loss_G: 3.6827\n",
            "[1/25][187/782] Loss_D: 0.6947 Loss_G: 4.8957\n",
            "[1/25][188/782] Loss_D: 0.9301 Loss_G: 2.1859\n",
            "[1/25][189/782] Loss_D: 1.2163 Loss_G: 7.0254\n",
            "[1/25][190/782] Loss_D: 0.9541 Loss_G: 4.1877\n",
            "[1/25][191/782] Loss_D: 0.3820 Loss_G: 3.0544\n",
            "[1/25][192/782] Loss_D: 0.5780 Loss_G: 4.8923\n",
            "[1/25][193/782] Loss_D: 0.3958 Loss_G: 4.0605\n",
            "[1/25][194/782] Loss_D: 0.5040 Loss_G: 4.4562\n",
            "[1/25][195/782] Loss_D: 0.6435 Loss_G: 5.8904\n",
            "[1/25][196/782] Loss_D: 1.0872 Loss_G: 1.3539\n",
            "[1/25][197/782] Loss_D: 2.5548 Loss_G: 11.6808\n",
            "[1/25][198/782] Loss_D: 3.1377 Loss_G: 6.7924\n",
            "[1/25][199/782] Loss_D: 0.4647 Loss_G: 3.3372\n",
            "[1/25][200/782] Loss_D: 0.7300 Loss_G: 3.6091\n",
            "[1/25][201/782] Loss_D: 0.5942 Loss_G: 6.1364\n",
            "[1/25][202/782] Loss_D: 0.5625 Loss_G: 4.0226\n",
            "[1/25][203/782] Loss_D: 0.3630 Loss_G: 3.9584\n",
            "[1/25][204/782] Loss_D: 0.6740 Loss_G: 5.6555\n",
            "[1/25][205/782] Loss_D: 0.5576 Loss_G: 3.8880\n",
            "[1/25][206/782] Loss_D: 0.5273 Loss_G: 4.6295\n",
            "[1/25][207/782] Loss_D: 0.7028 Loss_G: 4.8026\n",
            "[1/25][208/782] Loss_D: 0.4887 Loss_G: 4.8121\n",
            "[1/25][209/782] Loss_D: 0.4972 Loss_G: 4.7004\n",
            "[1/25][210/782] Loss_D: 0.7294 Loss_G: 4.3572\n",
            "[1/25][211/782] Loss_D: 0.7984 Loss_G: 3.5337\n",
            "[1/25][212/782] Loss_D: 0.5212 Loss_G: 5.4816\n",
            "[1/25][213/782] Loss_D: 0.2040 Loss_G: 5.5390\n",
            "[1/25][214/782] Loss_D: 0.3276 Loss_G: 3.8256\n",
            "[1/25][215/782] Loss_D: 0.4880 Loss_G: 2.8051\n",
            "[1/25][216/782] Loss_D: 0.7446 Loss_G: 5.5164\n",
            "[1/25][217/782] Loss_D: 1.7104 Loss_G: 2.0939\n",
            "[1/25][218/782] Loss_D: 1.4330 Loss_G: 5.8907\n",
            "[1/25][219/782] Loss_D: 0.5074 Loss_G: 5.3604\n",
            "[1/25][220/782] Loss_D: 0.2590 Loss_G: 3.0755\n",
            "[1/25][221/782] Loss_D: 0.7463 Loss_G: 5.1738\n",
            "[1/25][222/782] Loss_D: 0.4206 Loss_G: 5.0088\n",
            "[1/25][223/782] Loss_D: 0.6421 Loss_G: 3.1232\n",
            "[1/25][224/782] Loss_D: 1.1953 Loss_G: 6.1248\n",
            "[1/25][225/782] Loss_D: 0.9535 Loss_G: 3.4751\n",
            "[1/25][226/782] Loss_D: 0.7294 Loss_G: 5.4803\n",
            "[1/25][227/782] Loss_D: 0.6790 Loss_G: 2.6157\n",
            "[1/25][228/782] Loss_D: 0.9711 Loss_G: 7.0168\n",
            "[1/25][229/782] Loss_D: 0.8328 Loss_G: 1.0408\n",
            "[1/25][230/782] Loss_D: 1.4500 Loss_G: 9.2086\n",
            "[1/25][231/782] Loss_D: 1.8932 Loss_G: 2.8014\n",
            "[1/25][232/782] Loss_D: 0.5970 Loss_G: 2.1199\n",
            "[1/25][233/782] Loss_D: 0.5815 Loss_G: 4.3603\n",
            "[1/25][234/782] Loss_D: 1.1199 Loss_G: 1.5046\n",
            "[1/25][235/782] Loss_D: 1.1138 Loss_G: 3.7565\n",
            "[1/25][236/782] Loss_D: 0.9534 Loss_G: 2.2517\n",
            "[1/25][237/782] Loss_D: 0.7655 Loss_G: 3.0260\n",
            "[1/25][238/782] Loss_D: 0.9407 Loss_G: 2.4239\n",
            "[1/25][239/782] Loss_D: 0.6231 Loss_G: 2.6995\n",
            "[1/25][240/782] Loss_D: 0.7817 Loss_G: 3.5223\n",
            "[1/25][241/782] Loss_D: 0.7612 Loss_G: 2.3851\n",
            "[1/25][242/782] Loss_D: 0.4601 Loss_G: 3.4006\n",
            "[1/25][243/782] Loss_D: 0.4160 Loss_G: 3.4547\n",
            "[1/25][244/782] Loss_D: 0.3161 Loss_G: 3.4343\n",
            "[1/25][245/782] Loss_D: 0.4921 Loss_G: 3.4943\n",
            "[1/25][246/782] Loss_D: 0.3362 Loss_G: 3.3442\n",
            "[1/25][247/782] Loss_D: 0.4998 Loss_G: 3.1643\n",
            "[1/25][248/782] Loss_D: 0.4773 Loss_G: 2.9379\n",
            "[1/25][249/782] Loss_D: 0.5522 Loss_G: 2.8462\n",
            "[1/25][250/782] Loss_D: 0.5145 Loss_G: 3.0866\n",
            "[1/25][251/782] Loss_D: 0.4890 Loss_G: 3.6099\n",
            "[1/25][252/782] Loss_D: 0.6356 Loss_G: 2.0534\n",
            "[1/25][253/782] Loss_D: 0.9678 Loss_G: 5.0094\n",
            "[1/25][254/782] Loss_D: 1.1441 Loss_G: 2.2313\n",
            "[1/25][255/782] Loss_D: 0.6913 Loss_G: 4.0029\n",
            "[1/25][256/782] Loss_D: 0.6747 Loss_G: 2.9379\n",
            "[1/25][257/782] Loss_D: 0.3276 Loss_G: 3.1133\n",
            "[1/25][258/782] Loss_D: 0.4366 Loss_G: 4.0197\n",
            "[1/25][259/782] Loss_D: 0.5824 Loss_G: 3.1150\n",
            "[1/25][260/782] Loss_D: 0.2608 Loss_G: 2.8826\n",
            "[1/25][261/782] Loss_D: 0.5767 Loss_G: 4.4549\n",
            "[1/25][262/782] Loss_D: 0.5346 Loss_G: 3.0381\n",
            "[1/25][263/782] Loss_D: 0.3532 Loss_G: 3.6648\n",
            "[1/25][264/782] Loss_D: 0.4274 Loss_G: 2.8446\n",
            "[1/25][265/782] Loss_D: 0.5611 Loss_G: 4.6352\n",
            "[1/25][266/782] Loss_D: 0.6020 Loss_G: 2.7959\n",
            "[1/25][267/782] Loss_D: 0.3708 Loss_G: 3.4119\n",
            "[1/25][268/782] Loss_D: 0.3164 Loss_G: 3.5187\n",
            "[1/25][269/782] Loss_D: 0.1620 Loss_G: 3.8733\n",
            "[1/25][270/782] Loss_D: 0.3680 Loss_G: 3.2215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkXROWQIoeS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}